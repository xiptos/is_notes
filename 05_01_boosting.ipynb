{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n5ymNLE_uBu"
   },
   "source": [
    "\n",
    "# Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey5jcpme6Wi1",
    "outputId": "0f40efd0-f60f-4f3f-fdc8-2e12d95dd2e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWaSWzpR_uCd"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# join one or more path components intelligently\n",
    "from os.path import join\n",
    "\n",
    "# Interface com o sistema operacional\n",
    "import os\n",
    "\n",
    "# Manipulação de dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Manipulação de dados tabulares\n",
    "import numpy as np\n",
    "\n",
    "# visualização de dados baseada no matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# Esboço de gráficos\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Leitura de dados de arquivos hdf5\n",
    "import h5py\n",
    "\n",
    "# Separação do conjunto de dados em treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Normalização das características\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classificador AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Gradient Boosting para classificação\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Determinação dos conjuntos de treino e de validação cruzada para plotagem da curva de aprendizado\n",
    "from sklearn.model_selection import learning_curve     \n",
    "from sklearn.model_selection import ShuffleSplit    # Random permutation cross-validator\n",
    "\n",
    "# Classificadores Bagging e Random Forest\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "# Avaliação do modelo\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Busca exaustiva sobre valores de parâmetros especificados para um estimador (sintonização de hiperparâmetros)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Transformação das features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Classificador Multi-layer Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Warning messages\n",
    "import warnings\n",
    "\n",
    "# Leitura de dados de arquivo .mat\n",
    "from scipy.io import loadmat\n",
    "import scipy.io as spio\n",
    "\n",
    "# Otimização de funções\n",
    "from scipy import optimize as opt\n",
    "\n",
    "# Funções relacionadas ao tempo\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHSgcGEy_uCg"
   },
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Details**:\n",
    "\n",
    "- A (fictitious) financial institution has a database with the history of installment plans offered to its customers. \n",
    "\n",
    "- Based on the credit history offered to clients, the institution wants to investigate the creation of classification models to infer whether or not a new client who submitted a loan application will pay the debt, if the bank decides to take out this loan.\n",
    "\n",
    "- Objective: Predict whether or not a new customer would pay a contracted debt, based on the characteristics of this new customer. Once trained, a rating model for that problem can infer whether or not a new customer will honor an eventual loan granted to them.\n",
    "\n",
    "- The dataset to be used for training has 1500 examples and contains data related to credits (loans) granted to clients of the financial institution. Records of credits granted to financial institution customers are contained in the `credtrain.txt` file. For each customer, 11 attributes (variables, characteristics) are defined. In addition, the last column of each example tells you whether or not the customer has honored the loan payment.\n",
    "\n",
    "- Table 1 contains the description of the attributes.\n",
    "\n",
    "<img src=\"https://github.com/cristianegea/PPCIC/blob/main/Aprendizado%20de%20Máquinas/Trabalho%203/img/img1.png?raw=true\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Objectives**:\n",
    "\n",
    "- Perform experiments with *Boosting* algorithms to create classification models for the aforementioned problem. \n",
    "- Present the results (obtained with Scikit-Learn's `classification_report` function) for each classification model on the data contained in the `credtest.txt` file.\n",
    "\n",
    "Note: with respect to the model's hyperparameters, it is possible to use the values *default* or the values obtained by searching for hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Import and read data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "# Definition of variable names (according to the table contained in the statement)\n",
    "colnames = ['ESCT', 'NDEP', 'INCOME', 'TIPOR', 'VBEM', 'NPARC',\n",
    "            'VPARC', 'TEL', 'AGE', 'RESMS', 'ENTRY', 'CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8f0yXVbtEmZ"
   },
   "outputs": [],
   "source": [
    "# Reading training data\n",
    "file = 'datasets/credtrain.txt'\n",
    "data_train = pd.read_csv(file, sep='\\t', header=None, names = colnames)\n",
    "\n",
    "# Reading testing data\n",
    "file = 'datasets/credtest.txt'\n",
    "data_test = pd.read_csv(file, sep='\\t', header=None, names = colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Data inspection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Doy7gsSfecpv",
    "outputId": "b62a027b-6bad-4ae9-ee45-e8a26fc36a73"
   },
   "outputs": [],
   "source": [
    "# Dimension inspection\n",
    "print(data_train.shape, data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "1gM-thS-_uCj",
    "outputId": "665a5e52-1f28-4e04-fe02-a75ee0088e35"
   },
   "outputs": [],
   "source": [
    "# Training data structure\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "2qdSwb4AuGEo",
    "outputId": "14ce42a3-abb9-44c2-d2f6-c35519c12b47"
   },
   "outputs": [],
   "source": [
    "# Testing data structure\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Category variable treatment**\n",
    "\n",
    "It is important to note that the variable ESCT (Marial Status) is categorical and can take 4 different values ​​(each value corresponds to a marital status). Thus, unlike NDEP (where each value corresponds to a number of dependents), in the variable ESCT each value corresponds to a category. However, this fact can lead to inconsistencies in the creation and training of models.\n",
    "\n",
    "To mitigate this problem, an alternative is to transform the variable ESCT into a *dummy* variable (binary variable). In this sense, each category of the variable ESCT will correspond to a variable. Since there are 4 possible categories for the ESCT variable, we will get 4 binary ESCT variables.\n",
    "\n",
    "A *dummy* variable is a binary variable used to represent categories. In this sense, in a case of a variable with 3 or more categories, it is recommended to create $n-1$ dummies. Therefore, the variable ESCT will be transformed into 4 \"dummy variants\", where the value 1 will correspond to the occurrence of a certain category and the value 0 will correspond to the non-occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "yErjVPa6_uCo",
    "outputId": "6657ec51-40b2-4e52-fccb-f9912980e72b"
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "data_train_new = pd.get_dummies(data = data_train, \n",
    "                                prefix='ESCT', \n",
    "                                columns=['ESCT'], \n",
    "                                drop_first=True)\n",
    "\n",
    "\"\"\"\n",
    "pd.get_dummies: Convert categorical variable into dummy/indicator variables.\n",
    "\"\"\"\n",
    "\n",
    "data_train_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "hI9UgoSS_uCp",
    "outputId": "2c37fc2f-d01f-4be1-e34b-4fd6db65a466"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "data_test_new = pd.get_dummies(data = data_test, prefix='ESCT', columns=['ESCT'], drop_first=True)\n",
    "\n",
    "\"\"\"\n",
    "pd.get_dummies: Convert categorical variable into dummy/indicator variables\n",
    "\"\"\"\n",
    "\n",
    "data_test_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Separation of dataset into label ($\\mathrm{y}$) and features ($\\mathrm{x}$)**\n",
    "\n",
    "The label ($\\mathrm{y}$) corresponds to the vector containing the target variable (CLASS), while features ($\\mathrm{x}$) corresponds to the data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxKjqguU_uCr",
    "outputId": "350813e8-bb68-480c-b0ea-3cd0a7aa8d14"
   },
   "outputs": [],
   "source": [
    "# Transforming the target variable of the training set into vector\n",
    "y_train = np.array(data_train_new['CLASS'])\n",
    "\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bIZH23c_uCt",
    "outputId": "c90407cd-1373-4f24-c268-c5b29420b815"
   },
   "outputs": [],
   "source": [
    "# Transforming the target variable of the testing set into vector\n",
    "y_test = np.array(data_test_new['CLASS'])\n",
    "\n",
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7YkPky3_uCv",
    "outputId": "5d6fb232-9879-4518-b13a-271d7d88ebb1"
   },
   "outputs": [],
   "source": [
    "# Transforming the remaining training set into a data matrix\n",
    "features_name_train = list(data_train_new.columns)               \n",
    "features_name_train.remove('CLASS')                             \n",
    "X_train = np.array(data_train_new.loc[:, features_name_train])   \n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zl5JbSo2_uCw",
    "outputId": "802b1cd2-d415-4918-cefb-f359bba2494c"
   },
   "outputs": [],
   "source": [
    "# Transforming the remaining testing set into a data matrix\n",
    "features_name_test = list(data_test_new.columns)               \n",
    "features_name_test.remove('CLASS')                            \n",
    "X_test = np.array(data_test_new.loc[:, features_name_test])   \n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Important note**: Due to the fact that the `learning_curve` class itself performs the division of the dataset in training and validation to compare the learning curves, there is no need to divide the set of training in the training and validation subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Normalization of features**\n",
    "\n",
    "Similar to the previous works, before proceeding, the normalization of the characteristics will be carried out in order to avoid problems arising from the discrepancy in the order of magnitude of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNkkpj63EedQ"
   },
   "outputs": [],
   "source": [
    "# Object creation for feature standardization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Adjustment of StandardScaler to training dataset and standardization of training data\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transformation of test data with parameters adjusted from training data\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RazjkSXbe336",
    "outputId": "a97786c5-d666-4289-a01f-124ca35e7a64"
   },
   "outputs": [],
   "source": [
    "# Datasets dimension\n",
    "print(X_train_norm.shape, X_test_norm.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R_-WvuH_uC0"
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Objectives**:\n",
    "\n",
    "- Create classification models through the inductors: `sklearn.ensemble.AdaBoostClassifier` and `sklearn.ensemble.GradientBoostingClassifier`.\n",
    "- For each learning algorithm, present graphs that plot accuracy values (measured in the training and validation sets (against the number of training iterations).\n",
    "- Divide the examples contained in the `credtrain.txt` file into 2 subsets (training and validation), using the 80/20 ratio for training and validation, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Creating and training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**AdaBoost**\n",
    "\n",
    "AdaBoost uses the complete training set to train weak classifiers. In addition, the training samples are reweighted at each iteration to build a strong classifier, which learns from the mistakes previously made by the weak classifiers.\n",
    "\n",
    "This algorithm constructs a committee, $C^\\star(x)$, as a linear combination (weighted sum) of $T$ weak classifiers (weak learners).\n",
    "\n",
    "$$\n",
    "C^\\star(x) = \\sum^T_{i = 1} \\alpha_i C_i(x)\n",
    "$$\n",
    "\n",
    "where $c_i(x)$ is a weak classifier and $\\alpha_i$ is the weight assigned to each classifier/learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z96fsRSI_uC5"
   },
   "outputs": [],
   "source": [
    "# Creation of the AdaBoost inductor without changing hyperparameters\n",
    "adaboost = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7cc-F9z_uC6",
    "outputId": "6622548c-c53b-4bf4-ced0-6a30e5722dee"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "adaboost.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Gradient Boosting**\n",
    "\n",
    "Similar to AdaBoost, Gradient Boosting works by sequentially adding predictors to a committee, where each one fixes its predecessor. However, instead of adjusting the instance weights on each iteration (like AdaBoost), this method tries to adjust the new predictor to the residual errors made by the previous predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIDjB7Xm_uC9"
   },
   "outputs": [],
   "source": [
    "# Creation of the Gradient Boosting inductor without changing hyperparameters\n",
    "gradientboost = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C05C98bL_uC-",
    "outputId": "5d597b5b-3b09-4d53-ec00-ba57fa6aab50"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "gradientboost.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Learning curves visualize the performance of a model over the training set and during cross-validation, as the number of observations in the training set increases. They are commonly used to determine whether learning algorithms could benefit from collecting additional training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Definition of validation criteria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTOJcRj7_uDA"
   },
   "outputs": [],
   "source": [
    "# 20% random choice of set for validation for each iteration\n",
    "cv = ShuffleSplit(test_size = 0.2, random_state = 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**AdaBoost Model Learning Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71UqPsFM_uDC"
   },
   "outputs": [],
   "source": [
    "# Application of the function to compare validation and training results\n",
    "_, acc_treino, acc_val = learning_curve(adaboost,\n",
    "                                        X_train_norm,\n",
    "                                        y_train,\n",
    "                                        scoring = 'accuracy',\n",
    "                                        random_state = 31,\n",
    "                                        cv = cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sofNXk5I_uDD"
   },
   "outputs": [],
   "source": [
    "# Calculation of scores based on the average of the rounds\n",
    "acc_treino_adaboost = np.mean(acc_treino, axis=1)\n",
    "acc_val_adaboost = np.mean(acc_val, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "j4OerytbqLt4",
    "outputId": "c0e23c68-97e7-4bbd-af54-8b2382074d4b"
   },
   "outputs": [],
   "source": [
    "# learning curve graph\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(acc_treino_adaboost, alpha=0.7)         # Train\n",
    "plt.plot(acc_val_adaboost, 'g--', alpha=0.7)     # Validation\n",
    "\n",
    "# Graphics labels and captions\n",
    "plt.xlabel('Number of training examples', fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontweight='bold')\n",
    "plt.title('Learning curve: AdaBoost', fontweight='bold')\n",
    "plt.legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "From the graphic above, it is possible to observe that from 1 training example onwards, the algorithm is starting to adjust to the examples of the training and validation set. Therefore, it is possible to observe that the accuracy of the model with the training data presents a downward trend and the accuracy of the model with the validation data presents an upward trend. In other words, the graph above shows that as the number of training examples increases, the greater the training error tends to be and the smaller the validation error tends to be. Therefore, it is possible to observe a possible *underfitting* problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Gradient Boosting Model Learning Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOrR4r9u_uDF"
   },
   "outputs": [],
   "source": [
    "# Application of the function to compare validation and training results\n",
    "_, acc_treino, acc_val = learning_curve(gradientboost,\n",
    "                                        X_train_norm,\n",
    "                                        y_train,\n",
    "                                        scoring = 'accuracy',\n",
    "                                        random_state = 31,\n",
    "                                        cv = cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZd7404-_uDF"
   },
   "outputs": [],
   "source": [
    "# Calculation of scores based on the average of the rounds\n",
    "acc_treino_gradientboost = np.mean(acc_treino, axis=1)\n",
    "acc_val_gradientboost = np.mean(acc_val, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "h70x9D7Ps_Zc",
    "outputId": "b9c3e0dc-7304-4e93-ec8b-483883101e14"
   },
   "outputs": [],
   "source": [
    "# learning curve graph\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(acc_treino_gradientboost, alpha=0.7)         # Treino\n",
    "plt.plot(acc_val_gradientboost, 'g--', alpha=0.7)     # Validação\n",
    "\n",
    "# Graphics labels and captions\n",
    "plt.xlabel('Number of training examples', fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontweight='bold')\n",
    "plt.title('Learning curve: Gradient Boost', fontweight='bold')\n",
    "plt.legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "From the graphic above, it is possible to observe that from 1 training example onwards, the algorithm is starting to adjust to the examples of the training and validation set. Therefore, it is possible to observe that the accuracy of the model with the training data presents a downward trend and the accuracy of the model with the validation data presents an upward trend. In other words, the graph above shows that as the number of training examples increases, the greater the training error tends to be and the smaller the validation error tends to be. Therefore, it is possible to observe a possible *underfitting* problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Comparison between learning curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "Y20RLTna_uDG",
    "outputId": "c621b839-783a-4a0d-b7fe-021515f754d9"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "fig.suptitle('Comparison of Learning Curves', fontweight='bold')\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "# AdaBoost Learning Curve\n",
    "ax1.plot(acc_treino_adaboost, alpha=0.7)         # Train\n",
    "ax1.plot(acc_val_adaboost, 'g--', alpha=0.7)     # Validation\n",
    "\n",
    "# Graphics labels and captions\n",
    "ax1.set_xlabel('Number of training examples', fontweight='bold')\n",
    "ax1.set_ylabel('Acurácia', fontweight='bold')\n",
    "ax1.set_title('Learning curve: AdaBoost', fontweight='bold')\n",
    "\n",
    "\n",
    "# Gradient Boosting Learning Curve\n",
    "ax2.plot(acc_treino_gradientboost, alpha=0.7)         # Treino\n",
    "ax2.plot(acc_val_gradientboost, 'g--', alpha=0.7)     # Validação\n",
    "\n",
    "# Graphics labels and captions\n",
    "ax2.legend(['Treino', 'Validação'], loc='upper right')\n",
    "ax2.set_xlabel('Número de exemplos de treinamento', fontweight='bold')\n",
    "ax2.set_title('Curva de Aprendizado: Gradient Boosting', fontweight='bold')\n",
    "\n",
    "# Final visual adjustments\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "As discussed earlier for each case alone, both cases indicate a possible *underfitting* problem.\n",
    "\n",
    "Another point worth mentioning are the differences identified in each case:\n",
    "\n",
    "- While in AdaBoost it is possible to visualize a convergence between the training and validation curves, in Gradient Boost they are more distant.\n",
    "- The behavior of the training and validation curves have smoother trajectories for Gradient Boost than for AdaBoost.\n",
    "- The level of accuracy of the learning curves for Gradient Boost remained, on average, higher than that of the learning curves for AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Model Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl43I7tXVB0T"
   },
   "source": [
    "**AdaBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auo1wZ_e_uDJ"
   },
   "outputs": [],
   "source": [
    "# Using models for prediction\n",
    "y_pred_adaboost = adaboost.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rV34BU1M_uDK",
    "outputId": "2c9afcfe-2f21-407e-e13c-cfc3ff15e02c"
   },
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "print(classification_report(y_test, y_pred_adaboost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPWyUkpzVxVS"
   },
   "source": [
    "**Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1tR9h5l_uDL"
   },
   "outputs": [],
   "source": [
    "# Using models for prediction\n",
    "y_pred_gradientboost = gradientboost.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYYwDQls_uDM",
    "outputId": "9fd28dfe-1b55-45d3-e99c-6213a13d5eb1"
   },
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "print(classification_report(y_test, y_pred_gradientboost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Comparison of performance between models**\n",
    "\n",
    "From the comparison above, it is possible to infer that Gradient Boosting had better predictive performance compared to AdaBoost. Given that the Gradient Boosting learning curves presented a more stable behavior and at a higher level of accuracy, it was expected that it would present a superior predictive performance (compared to the AdaBoost model)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6n5ymNLE_uBu"
   ],
   "name": "Versão_Full.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "pt",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
