{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiptos/is_notes/blob/main/word_embeddings_cbow.ipynb)",
   "id": "3a4922a97ee59c15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Mini Word2Vec (CBOW) in PyTorch\n",
    "\n",
    "Goal:\n",
    "- Build word embeddings from a tiny corpus\n",
    "- Use a CBOW-style model:\n",
    "    given context words → predict the center word\n",
    "- Then inspect which words end up close in the embedding space\n",
    "\n",
    "This is a teaching example: small, simple, and fast."
   ],
   "id": "e623dfd18f3ec9b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n"
   ],
   "id": "195448c5eceabf03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 1. Toy corpus\n",
    "Small corpus with clear semantic structure (royalty vs fruit vs places, etc.)"
   ],
   "id": "955583ae7f69029c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "corpus = [\n",
    "    \"king is a strong man\",\n",
    "    \"queen is a wise woman\",\n",
    "    \"man and woman are humans\",\n",
    "    \"king and queen rule the kingdom\",\n",
    "    \"paris is a city\",\n",
    "    \"france is a country\",\n",
    "    \"paris is in france\",\n",
    "    \"apple and orange are fruits\",\n",
    "    \"apple is red\",\n",
    "    \"orange is orange\",\n",
    "]\n",
    "\n",
    "corpus = [s.lower().split() for s in corpus]\n",
    "print(corpus[:2])"
   ],
   "id": "64b931bba3e4d260",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(list(word for sent in corpus for word in sent))",
   "id": "28b57d433160d013",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Build vocabulary and mappings",
   "id": "5612ff1f6ca742aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Count words\n",
    "word_counts = Counter(word for sent in corpus for word in sent)\n",
    "vocab = sorted(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Vocab:\", vocab)"
   ],
   "id": "23f3ff44fcdb5e5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Generate CBOW training data\n",
    "\n",
    "For window size = 2:\n",
    "- Input: the 4 context words (2 left, 2 right)\n",
    "- Target: the center word"
   ],
   "id": "c3f2509140a31e48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "WINDOW_SIZE = 2\n",
    "\n",
    "def make_cbow_data(corpus, window_size=2):\n",
    "    data = []\n",
    "    for sent in corpus:\n",
    "        indices = [word_to_idx[w] for w in sent]\n",
    "        for i in range(window_size, len(indices) - window_size):\n",
    "            context = (\n",
    "                indices[i - window_size : i] +\n",
    "                indices[i + 1 : i + 1 + window_size]\n",
    "            )\n",
    "            target = indices[i]\n",
    "            data.append((context, target))\n",
    "    return data\n",
    "\n",
    "data = make_cbow_data(corpus, WINDOW_SIZE)\n",
    "print(\"Number of training examples:\", len(data))\n",
    "print(\"Example (context idx, target idx):\", data[0])\n",
    "\n",
    "# Show example in words\n",
    "ctx_words = [idx_to_word[i] for i in data[0][0]]\n",
    "tgt_word = idx_to_word[data[0][1]]\n",
    "print(\"Context words:\", ctx_words)\n",
    "print(\"Target word:\", tgt_word)"
   ],
   "id": "49c7c9b1f73daa2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Define CBOW model with an Embedding layer\n",
    "\n",
    "- `nn.Embedding` maps word indices → dense vectors\n",
    "- We average the context embeddings, then use a linear layer to predict the target word"
   ],
   "id": "7a5fd768884934f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_idxs):\n",
    "        \"\"\"\n",
    "        context_idxs: LongTensor of shape (batch_size, 2*window_size)\n",
    "        \"\"\"\n",
    "        # Get embeddings: (batch, 2*window, embed_dim)\n",
    "        embeds = self.emb(context_idxs)\n",
    "        # Average over context words: (batch, embed_dim)\n",
    "        pooled = embeds.mean(dim=1)\n",
    "        # Predict vocabulary distribution\n",
    "        logits = self.linear(pooled)\n",
    "        return logits\n",
    "\n",
    "EMBED_DIM = 10  # small dimension for visualization\n",
    "model = CBOW(vocab_size, EMBED_DIM)\n",
    "print(model)"
   ],
   "id": "df2ccb678aee98ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Training loop\n",
    "\n",
    "- Loss: cross-entropy between predicted distribution and true target word\n",
    "- Tiny corpus → few epochs are enough to see structure"
   ],
   "id": "ec4297c602315f3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "lr = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def get_batches(data, batch_size):\n",
    "    random.shuffle(data)\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i : i + batch_size]\n",
    "        contexts = torch.tensor([c for (c, t) in batch], dtype=torch.long)\n",
    "        targets = torch.tensor([t for (c, t) in batch], dtype=torch.long)\n",
    "        yield contexts, targets\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0.0\n",
    "    for contexts, targets in get_batches(data, BATCH_SIZE):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(contexts)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * contexts.size(0)\n",
    "    if epoch % 40 == 0:\n",
    "        avg_loss = total_loss / len(data)\n",
    "        print(f\"Epoch {epoch:3d} | loss = {avg_loss:.4f}\")"
   ],
   "id": "b468f16bcceefb4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Inspect learned embeddings\n",
    "\n",
    "Let's:\n",
    "- Extract the embedding matrix\n",
    "- Define a helper to get nearest neighbors (cosine similarity)"
   ],
   "id": "dd03200a3cea6bc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "embeddings = model.emb.weight.data  # shape: (vocab_size, EMBED_DIM)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a_norm = a / (a.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "    b_norm = b / (b.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "    return (a_norm * b_norm).sum(dim=-1)\n",
    "\n",
    "def most_similar(word, top_k=5):\n",
    "    if word not in word_to_idx:\n",
    "        print(f\"'{word}' not in vocab\")\n",
    "        return\n",
    "    idx = word_to_idx[word]\n",
    "    word_vec = embeddings[idx].unsqueeze(0)  # (1, D)\n",
    "    sims = cosine_similarity(word_vec, embeddings)  # (V,)\n",
    "    # Sort by similarity, skip the word itself (idx)\n",
    "    best = torch.topk(sims, top_k + 1).indices.tolist()\n",
    "    best = [i for i in best if i != idx][:top_k]\n",
    "    print(f\"\\nMost similar to '{word}':\")\n",
    "    for i in best:\n",
    "        print(f\"  {idx_to_word[i]:>8s}  (cosine={sims[i]:.3f})\")"
   ],
   "id": "3b92f215dfafb0d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "most_similar(\"king\")\n",
    "most_similar(\"queen\")\n",
    "most_similar(\"apple\")\n",
    "most_similar(\"paris\")\n",
    "most_similar(\"france\")"
   ],
   "id": "c92f218bcc8c7581",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. (Optional) Analogy intuition\n",
    "\n",
    "Very tiny corpus → analogies will be noisy, but we can still show the idea:\n",
    "vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\")"
   ],
   "id": "1d961f57349cbec0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def analogy(a, b, c, top_k=5):\n",
    "    for w in (a, b, c):\n",
    "        if w not in word_to_idx:\n",
    "            print(f\"'{w}' not in vocab\")\n",
    "            return\n",
    "    va = embeddings[word_to_idx[a]]\n",
    "    vb = embeddings[word_to_idx[b]]\n",
    "    vc = embeddings[word_to_idx[c]]\n",
    "    query = vb - va + vc  # b - a + c\n",
    "\n",
    "    sims = cosine_similarity(query.unsqueeze(0), embeddings)\n",
    "    best = torch.topk(sims, top_k + 3).indices.tolist()\n",
    "    # Filter out the original words\n",
    "    exclude = {word_to_idx[w] for w in (a, b, c)}\n",
    "    best = [i for i in best if i not in exclude][:top_k]\n",
    "\n",
    "    print(f\"\\nAnalogy: {a} → {b}  as  {c} → ?\")\n",
    "    for i in best:\n",
    "        print(f\"  {idx_to_word[i]:>8s}  (cosine={sims[i]:.3f})\")\n",
    "\n",
    "analogy(\"man\", \"king\", \"woman\")"
   ],
   "id": "55bccc7d71abaa95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "afdbd468e31449e3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
