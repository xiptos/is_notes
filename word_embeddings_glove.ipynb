{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T15:15:34.156242Z",
     "start_time": "2025-11-30T15:15:34.155116Z"
    }
   },
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiptos/is_notes/blob/main/word_embeddings_glove.ipynb)",
   "id": "d39c61c9c3ceba80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Set the dependencies and download\n",
    "\n",
    "We start by importing the dependencies and download the _GloVe_ embeddings"
   ],
   "id": "8e42f9f6dadf03b2"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Directory for GloVe\n",
    "glove_dir = \"glove\"\n",
    "zip_path = os.path.join(glove_dir, \"glove.6B.zip\")\n",
    "\n",
    "# Files we expect after extraction\n",
    "expected_files = [\n",
    "    \"glove.6B.50d.txt\",\n",
    "    \"glove.6B.100d.txt\",\n",
    "    \"glove.6B.200d.txt\",\n",
    "    \"glove.6B.300d.txt\",\n",
    "]\n",
    "\n",
    "# 1. Create directory if missing\n",
    "os.makedirs(glove_dir, exist_ok=True)\n",
    "\n",
    "# 2. Check if the ZIP file exists\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"GloVe ZIP not found. Downloading…\")\n",
    "    url = \"https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\"\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"ZIP file already exists — skipping download.\")\n",
    "\n",
    "# 3. Check if files are already extracted\n",
    "if not all(os.path.exists(os.path.join(glove_dir, f)) for f in expected_files):\n",
    "    print(\"Extracting GloVe files…\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(glove_dir)\n",
    "    print(\"Extraction complete.\")\n",
    "else:\n",
    "    print(\"GloVe text files already extracted — skipping extraction.\")"
   ],
   "id": "b904f0e5dbe09a22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load and use the embeddings\n",
    "\n",
    "Load the 50 dimension vector embedding."
   ],
   "id": "34cdd56e4637c15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove(path):\n",
    "    word2vec = {}\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=float)\n",
    "            word2vec[word] = vector\n",
    "    return word2vec\n",
    "\n",
    "glove50 = load_glove(\"glove/glove.6B.50d.txt\")\n",
    "print(\"Loaded vocabulary size:\", len(glove50))"
   ],
   "id": "9862e0c50522f1ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T15:32:35.561770Z",
     "start_time": "2025-11-30T15:32:35.560266Z"
    }
   },
   "cell_type": "markdown",
   "source": "## Build the matrix and index",
   "id": "b35fe9ee56188760"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Build a matrix and index mapping\n",
    "vocab = list(glove50.keys())\n",
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "embedding_matrix = torch.tensor(\n",
    "    np.stack([glove50[w] for w in vocab]),\n",
    "    dtype=torch.float\n",
    ")\n",
    "\n",
    "embedding_matrix = embedding_matrix / embedding_matrix.norm(dim=1, keepdim=True)\n",
    "print(embedding_matrix.shape)"
   ],
   "id": "ec38a3366a1dbb2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Some auxiliarey functions\n",
    "\n",
    "To find the most similar word"
   ],
   "id": "3ac2f1df2aad5f86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_vector(word):\n",
    "    if word not in word_to_idx:\n",
    "        raise ValueError(f\"'{word}' not in vocabulary\")\n",
    "    return embedding_matrix[word_to_idx[word]]\n",
    "\n",
    "def most_similar(word, top_k=10):\n",
    "    v = get_vector(word)\n",
    "    sims = torch.mv(embedding_matrix, v)\n",
    "    best = torch.topk(sims, top_k + 1).indices.tolist()\n",
    "    best = [i for i in best if vocab[i] != word][:top_k]\n",
    "    print(f\"\\nWords most similar to '{word}':\")\n",
    "    for i in best:\n",
    "        print(f\"{vocab[i]:>10s}   (cosine={sims[i]:.3f})\")"
   ],
   "id": "7e18d68dbb427940",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "most_similar(\"king\")\n",
    "most_similar(\"paris\")\n",
    "most_similar(\"apple\")"
   ],
   "id": "e9f8f3ed9504bf53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Find analogies",
   "id": "7fa419c1f6bea7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def analogy(a, b, c, top_k=5):\n",
    "    for w in [a, b, c]:\n",
    "        if w not in word_to_idx:\n",
    "            print(f\"'{w}' not in vocab\")\n",
    "            return\n",
    "    va = get_vector(a)\n",
    "    vb = get_vector(b)\n",
    "    vc = get_vector(c)\n",
    "\n",
    "    query = vb - va + vc\n",
    "    # Normalize\n",
    "    query = query / query.norm()\n",
    "\n",
    "    sims = torch.mv(embedding_matrix, query)\n",
    "    best = torch.topk(sims, top_k + 3).indices.tolist()\n",
    "    exclude = {word_to_idx[w] for w in [a, b, c]}\n",
    "    best = [i for i in best if i not in exclude][:top_k]\n",
    "\n",
    "    print(f\"\\nAnalogy: {a} → {b} :: {c} → ?\")\n",
    "    for i in best:\n",
    "        print(f\"{vocab[i]:>10s}   (cosine={sims[i]:.3f})\")"
   ],
   "id": "d2df2ab35aba2c2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "analogy(\"man\", \"king\", \"woman\")\n",
    "analogy(\"paris\", \"france\", \"rome\")\n",
    "analogy(\"big\", \"bigger\", \"small\")"
   ],
   "id": "1d1dd672237d6505",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a74d3a6e8f6d3efb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
