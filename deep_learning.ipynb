{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiptos/is_notes/blob/main/deep_learning.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0a910ec6c8ad17a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54b6fc31-0725-4de7-aa6e-474328d993aa"
  },
  {
   "cell_type": "markdown",
   "id": "c26f540e-af08-495f-b096-ab1fa3ffdc7a",
   "metadata": {},
   "source": [
    "## Vectorization examples\n",
    "\n",
    "Let's start by importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6116793-93b3-414c-95c3-cdeb99034fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2697d3f6-4e50-45fa-85f9-244add7f4ef3",
   "metadata": {},
   "source": [
    "For flexibility, we can generate a random classification dataset, with two features and 100 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124da025-8018-4a56-89b8-5d2f21895b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.title(\"One informative feature, one cluster per class\", fontsize=\"small\")\n",
    "X1, Y1 = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=1, n_clusters_per_class=1\n",
    ")\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker=\"o\", c=Y1, s=25, edgecolor=\"k\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2be69b-9f8a-453f-aa47-e5464c9a2bf8",
   "metadata": {},
   "source": [
    "Let's confirm the shape of the examples and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d029eb-7e09-40a4-9a32-2d8f0a432618",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X1.shape)\n",
    "print(Y1.shape)\n",
    "print(X1)\n",
    "print(Y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2a81e-59ff-4160-9d40-6f8da4c2b08d",
   "metadata": {},
   "source": [
    "To be able to use a PyTorch model, it is necessary to convert the numpy arrays to torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21709461-3681-487f-a796-30ce63b5884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tensors\n",
    "X_train=torch.FloatTensor(X1)\n",
    "y_train=torch.LongTensor(Y1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b98f3-0bd3-454f-bd3b-d0906e2ca30c",
   "metadata": {},
   "source": [
    "## Building and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ccc4e-fafd-4f7f-9a41-b58f8671e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Model\n",
    "class ANN_model(nn.Module):\n",
    "    def __init__(self,input_features=2,hidden1=20, hidden2=10,out_features=2):\n",
    "        super().__init__()\n",
    "        self.f_connected1 = nn.Linear(input_features,hidden1)\n",
    "        self.f_connected2 = nn.Linear(hidden1,hidden2)\n",
    "        self.out = nn.Linear(hidden2,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.sigmoid(self.f_connected1(x))\n",
    "        x = torch.sigmoid(self.f_connected2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8992c5-4cd4-49f4-9aa9-552a9dbab97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500\n",
    "\n",
    "\n",
    "torch.manual_seed(20)\n",
    "model = ANN_model()\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aad991-bfe7-4210-8186-51a51d83d5ce",
   "metadata": {},
   "source": [
    "We have to define the loss function as well as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb812f1-c226-4dd1-a3b8-430c09d531c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Propergation - loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50bae0e-0d76-4d6e-bc7b-c4c3d6c9aebd",
   "metadata": {},
   "source": [
    "Now we can proceed with the training.\n",
    "\n",
    "The key concept here is the batch size... think about it and experiment with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9cf0cd-4f1d-4dcf-8bb2-d76f4593caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_losses=[]\n",
    "batch_size=1\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # X is a torch Variable\n",
    "    permutation = torch.randperm(X_train.size()[0])\n",
    "\n",
    "    for i in range(0,X_train.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "\n",
    "        # in case you wanted a semi-full example\n",
    "        if epoch==0 and i==0:\n",
    "            print(\"===> \" + str(batch_x.shape))\n",
    "\n",
    "        outputs = model.forward(batch_x)\n",
    "        loss = loss_function(outputs,batch_y)\n",
    "        final_losses.append(loss)\n",
    "        if epoch % 100 == 1:\n",
    "            print(\"Epoch number: {} and the loss : {}\".format(i,loss.item()))\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df1bbf5-3107-4f02-aa40-e3fd9e94cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the loss function\n",
    "final_losses_np = []\n",
    "for l in final_losses:\n",
    "    final_losses_np.append(l.detach().numpy())\n",
    "\n",
    "plt.plot(range(int(epochs*100/batch_size)),final_losses_np)\n",
    "plt.ylabel('Loss')\n",
    "plt.ylabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35571bd2-f0b2-4c5e-88d7-b6182ea41810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
