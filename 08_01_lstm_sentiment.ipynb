{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbdf7ced",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff7c7b2",
   "metadata": {},
   "source": [
    "The current notebook performs sentiment analysis on the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28149d11",
   "metadata": {},
   "source": [
    "First, it is necessary to load the required libraries. If not already available, install `opendatasets`, to allow downloading datasets from Kaggle with `!pip install opendatasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3edc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487ea81",
   "metadata": {},
   "source": [
    "If cuda is available, let's use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd2bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32788de4",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a2da1",
   "metadata": {},
   "source": [
    "Download the IMDB dataset from kaggle.\n",
    "**You need to input your Kaggle username and key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "od.download(\"https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71d901",
   "metadata": {},
   "source": [
    "Let's take a quick look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_csv = 'imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\n",
    "df = pd.read_csv(base_csv)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdb55a2",
   "metadata": {},
   "source": [
    "### Spliting the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051bf6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df['review'].values,df['sentiment'].values\n",
    "x_train_sentence,x_test_sentence,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "print(f'shape of train data is {x_train_sentence.shape}')\n",
    "print(f'shape of test data is {x_test_sentence.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b225047e",
   "metadata": {},
   "source": [
    "## Tokenizing the dataset\n",
    "\n",
    "Before using the dataset, it is necessary to convert the sentences into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9bb545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "def tokenize(x_train,y_train,x_val,y_val):\n",
    "    word_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for sent in x_train:\n",
    "        for word in sent.lower().split():\n",
    "            word = preprocess_string(word)\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "  \n",
    "    corpus = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n",
    "    # creating a dict\n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "    \n",
    "    # tokenize\n",
    "    final_list_train,final_list_test = [],[]\n",
    "    for sent in x_train:\n",
    "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
    "    for sent in x_val:\n",
    "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
    "            \n",
    "    encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n",
    "    encoded_test = [1 if label =='positive' else 0 for label in y_val] \n",
    "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "x_train,y_train,x_test,y_test,vocab = tokenize(x_train_sentence,y_train,x_test_sentence,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ff179",
   "metadata": {},
   "source": [
    "Lets compare a sentences before and after pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccefd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])\n",
    "print(x_train_sentence[0])\n",
    "sentence = []\n",
    "for i in x_train[0]:\n",
    "    for key, value in vocab.items():\n",
    "        if i == value:\n",
    "            sentence.append(key)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Length of vocabulary is {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e48966",
   "metadata": {},
   "source": [
    "It is not necessary to pad sequences in RNNs. However, we have to do it to be able to use batch training. How can we define the padding length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1efbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfbde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have very less number of reviews with length > 500.\n",
    "#So we will consideronly those below it.\n",
    "x_train_pad = padding_(x_train,500)\n",
    "x_test_pad = padding_(x_test,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0d8f2",
   "metadata": {},
   "source": [
    "Let's compare one sentence before and after padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])\n",
    "print(x_train_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c80062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0381de3",
   "metadata": {},
   "source": [
    "## The LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd50fe",
   "metadata": {},
   "source": [
    "Please, pay attention to the architecture, in particular the Embedding layer. What is the purpose of this layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 64]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6404799b",
   "metadata": {},
   "source": [
    "Creating the object..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9390850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_layers = 2\n",
    "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "\n",
    "model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d30f69d",
   "metadata": {},
   "source": [
    "Loss and optimization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19099aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d084f0",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e33255",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = 5\n",
    "epochs = 5 \n",
    "valid_loss_min = np.Inf\n",
    "# train for some number of epochs\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    # initialize hidden state \n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output,h = model(inputs,h)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        # calculating accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    " \n",
    "    \n",
    "        \n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "            val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            accuracy = acc(output,labels)\n",
    "            val_acc += accuracy\n",
    "            \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), 'state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c272fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_tr_acc, label='Train Acc')\n",
    "plt.plot(epoch_vl_acc, label='Validation Acc')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_tr_loss, label='Train loss')\n",
    "plt.plot(epoch_vl_loss, label='Validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75942ea0",
   "metadata": {},
   "source": [
    "## Predict sentiment\n",
    "\n",
    "Now let's check the model with a couple of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3dd515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text):\n",
    "        word_seq = np.array([vocab[preprocess_string(word)] for word in text.split() \n",
    "                         if preprocess_string(word) in vocab.keys()])\n",
    "        word_seq = np.expand_dims(word_seq,axis=0)\n",
    "        pad =  torch.from_numpy(padding_(word_seq,500))\n",
    "        inputs = pad.to(device)\n",
    "        batch_size = 1\n",
    "        h = model.init_hidden(batch_size)\n",
    "        h = tuple([each.data for each in h])\n",
    "        output, h = model(inputs, h)\n",
    "        return(output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d961234",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 30\n",
    "print(df['review'][index])\n",
    "print('='*70)\n",
    "print(f'Actual sentiment is  : {df[\"sentiment\"][index]}')\n",
    "print('='*70)\n",
    "pro = predict_text(df['review'][index])\n",
    "status = \"positive\" if pro > 0.5 else \"negative\"\n",
    "pro = (1 - pro) if status == \"negative\" else pro\n",
    "print(f'Predicted sentiment is {status} with a probability of {pro}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a593a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 32\n",
    "print(df['review'][index])\n",
    "print('='*70)\n",
    "print(f'Actual sentiment is  : {df[\"sentiment\"][index]}')\n",
    "print('='*70)\n",
    "pro = predict_text(df['review'][index])\n",
    "status = \"positive\" if pro > 0.5 else \"negative\"\n",
    "pro = (1 - pro) if status == \"negative\" else pro\n",
    "print(f'predicted sentiment is {status} with a probability of {pro}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
