{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0611d965",
   "metadata": {},
   "source": [
    "# Convolution Autoencoders (CAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c136ea1e",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8ac874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d6e0d",
   "metadata": {},
   "source": [
    "Utility functions, to store the temporary images and to convert matrix shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31d30d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')\n",
    "    \n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b169be",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c21a0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Transforms images to a PyTorch Tensor\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5)\n",
    "])\n",
    "\n",
    "# Download the MNIST Dataset\n",
    "dataset = datasets.MNIST(root = \"./data\", train = True,download = True,transform = img_transform)\n",
    "\n",
    "# DataLoader is used to load the dataset\n",
    "# for training\n",
    "loader = torch.utils.data.DataLoader(dataset = dataset,batch_size = batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e049be",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7911d817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4753c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a PyTorch class\n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 784 ==> 9\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1,16,3),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16,8,3),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(8,3,3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            torch.nn.Conv2d(3,16,3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            torch.nn.Conv2d(16,1,3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded,decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e637c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = AE()\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "# Using an Adam Optimizer with lr = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate,weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6523df8b",
   "metadata": {},
   "source": [
    "## Training/Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "094eebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"cae_3.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4b9b938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([128, 1, 28, 28])) that is different to the input size (torch.Size([128, 1, 6, 6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (28) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98190/4133819342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Calculating the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# The gradients are set to zero,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (28) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "if exists(model_file):\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()\n",
    "else:\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in loader:\n",
    "            image, label = data\n",
    "\n",
    "            # Reshaping the image to (-1, 784)\n",
    "#            image = image.view(image.size(0), -1)\n",
    "            #image = image.reshape(-1, 28*28)\n",
    "\n",
    "            # Output of Autoencoder\n",
    "            image = image.to(device)\n",
    "\n",
    "            _,reconstructed = model(image)\n",
    "\n",
    "            # Calculating the loss function\n",
    "            loss = loss_function(reconstructed, image)\n",
    "\n",
    "            # The gradients are set to zero,\n",
    "            # the the gradient is computed and stored.\n",
    "            # .step() performs parameter update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Storing the losses in a list for plotting\n",
    "            losses.append(loss)\n",
    "        print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.data))\n",
    "        if epoch % 10 == 0:\n",
    "            pic = to_img(reconstructed.cpu().data)\n",
    "            save_image(pic, './mlp_img/image_{}.png'.format(epoch))\n",
    "\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "    model.eval()\n",
    "    # Defining the Plot Style\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    # Plotting the last 100 values\n",
    "    plt.plot(losses[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a709fb",
   "metadata": {},
   "source": [
    "## Display sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1476c4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAEKCAYAAADn1WuOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcUUlEQVR4nO3dbWxTV5oH8P+1nRjnBRyCcQIkYYY4SkJBKWFgRRdawogOS0tgSqcw3Q9QqDoZRi1S6Qaj2SLRipAW0FAJRUhZph9AM1TB0lI0A+0sQS00hH0pS2dgdlIyJVBCQkJM4iTOi333A8KJY+ecA3GwE/4/Ccn3nOt7jq/D4+N7H5+jud1uHURECgzR7gARjR0MGESkjAGDiJQxYBCRMgYMIlLGgEFEyhgwiEhZRANGZWUl5s6dC7vdjmeffRZfffVVJA9PRFEWsYDhcrmwfft2vP322/jiiy+wYMECvPzyy7hx40akmiCiKNMilem5bNkyzJ49Gx999FGgbN68eSguLsbOnTtD9l+bsjnwuOxiKZwLyiPRjScez2XkPKnnsqqtcti6iIwwent7cenSJRQVFQWVFxUVoba2NhJNEFEMMEXiIK2trfD5fLDZbEHlNpsNzc3NYZ9TdrFUuE2Pjucycngug0UkYDygaVrQtq7rIWUPDB7qPalDv9HAcxk5T+q5HPWvJKmpqTAajSGjiZaWlpBRBxGNXREJGPHx8SgoKEB1dXVQeXV1NRYuXBiJJogoBkTsK8mWLVvwxhtvoLCwEAsXLsThw4dx+/ZtbNy4MVJNEFGURSxg/PSnP8Xdu3fx4YcfoqmpCXl5efjkk0+QmZkZqSaIKMoietFz8+bN2Lx5s3xHIhqT+FsSIlLGgEFEyhgwiEgZAwYRKWPAICJlEb1LQjFomNT8gXr5Z4ZmGPkxpGRtyPgVfnSt+yXVYY5hMCo///4+kn7I3g8VsjZGEUcYRKSMAYOIlDFgEJEyBgwiUsaAQUTKGDCISBkDBhEpYx5GLFO4Z68ZjaFlJlPYx2GfP8Es70dc/MiOYQrt41C6UfzZpXX3iJ/v9UrbQF+/+Bi9vSFlhvi4gfp+8fMBQPf5JDtEL4ciEjjCICJlDBhEpIwBg4iUMWAQkTIGDCJSxoBBRMoYMIhIGfMwHlUk5pmIE59+g1meI6ElJoSUGW1TAo/1iUnC5/dPTpS24Z0q7kfXVHGehc8szycxesX5CeZ74rkoLC198jY6xfuYmtwhZQb7wMp9ets9aRv+Hkm+iCQX5P5BJLkcMipzbjxiPghHGESkjAGDiJQxYBCRMgYMIlLGgEFEyhgwiEgZAwYRKWPAICJlTNwajiT5RTPFievjxfUAoCWEJl0FmTxJeozetOTQspz0wGPPdHHSlTtb/pnhzQqdWGawnJm3hPVxRnkiUoPbKqxvbRInoJnuyZPcEm9OENZPbLCElHU8PXAuk67K2zDe6xDW+zu7pMfQu7vF9bKJfEZxkp6IjDDKyspgtVqD/uXk5ETi0EQUQyI2wnA4HDh58mRg2xhm6jgiGtsiFjBMJhPsdnukDkdEMShiFz2/++475OXlYe7cuXjttdfw3XffRerQRBQjNLfbPeIrJJ9//jk8Hg8cDgdaWlrw4Ycfoq6uDhcuXMDkyZPDPqeurm6kzRLRKHA4HMPWRSRgDOXxeFBQUICtW7fiV7/6Vdh91qZsDjwuu1gK54LySHdjZMbIXZK+IXdJyj9cjtJ3PgtsP567JLeF9ZG4S9IhvUsi/3adeFP8nk5sCL778JsdS7F1d3VgO+nqXWkbWizcJRmhqrbKYetGJQ8jKSkJubm5qK+vH43DE1GUjEoehtfrRV1dHRYvXjwah388JBPgaLKFd+LFi/8AAKZYhdVdM8X1ANCeFfoWtuYN5Bu0Z4sHkAnZbdI2/inj/4T1z078q7B+VlyrtI3/9GYJ669mTRPWX7o7Q9rG36zpwnrfhNBRYVvOwPk19KRI20j4VjzRj6YyOugTT/QTzcWSIhIwfv3rX+MnP/kJZsyYEbiG0dXVhfXr10fi8EQUIyISMG7duoXNmzejtbUVU6ZMwfz58/H5558jMzMzEocnohgRkYBx+PDhSByGiGIcf3xGRMoYMIhIGQMGESljwCAiZU/mfBgG+S9pDZJMTc0SOndCUP2k0HkqhuqdKs5evPcDebZox6zQ+/7tjoH78DPmiLMwfzTlurSN11PPCetz4mSLIYnPFQAkG8RJfost4vpP45+SttHZJ86NudVrCynrzBzIeYjrkP93iW+fKKyP6/JKj6F3i/fRJL8EH81MUI4wiEgZAwYRKWPAICJlDBhEpIwBg4iUMWAQkTIGDCJSxoBBRMrGZ+KWbHq9OPnL1iaIp7aTJWb5pogTeACgY4a4jc5M+UQoEx2hE+AMLvvBRPHkNbmWRmkbRoj7cbPfI6yPl7wfAFDfJz5fnbo46arHL09yc1jvCOtvTw7tg2FyT+Bxv2xKRQC9E8X9iDPIP6OliVnSI4wejjCISBkDBhEpY8AgImUMGESkjAGDiJQxYBCRMgYMIlI2PvMwJDSTwss2i3MkdLM4L8A3Qd6Gd7I4Xvelihe0AQBbYqewzGIUH+OuTzb5DVDV/rSwvqVPPBHQjW75AkDtvROE9dZ48fKBsnoAiDeIJ5aJiwtdIGhwmUG8YiQAwNgrXsgIfkk9AL1X3JDuj14mBkcYRKSMAYOIlDFgEJEyBgwiUsaAQUTKGDCISBkDBhEpY8AgImXjM3FLi34c9E2Qr67mE+cqwZQoT9yymEL3GVzW4xO/xa4bBdI2PF5xEltXp7je36XwZxYnTmhKSRVP0vOP08QrowGAu1ecYOZtD30dg8sSu+UJU6ZO8Xum9/QI6wGFlct0efLXaFH6n3X+/HmsW7cOeXl5sFqtOHr0aFC9rusoKytDbm4u0tLSsHLlSly9enVUOkxE0aMUMDo7O5Gfn489e/bAEmZN0QMHDuDgwYMoLy/HmTNnYLPZsGbNGnR0dES8w0QUPUoBY/ny5Xj33XdRXFwMw5A5CXVdR0VFBbZu3Yri4mLk5+ejoqICHo8HVVVVo9JpIoqOEX/Zv379OpqamlBUVBQos1gsWLRoEWpra0d6eCKKISO+6NnU1AQAsNlsQeU2mw2NjcPPSF12sVS4TY/uXcsG9Z1TR60bj5fKdUDZxOJzQouOz1kmrH/SROwuiTZkKnld10PKBnMuKA88LrtYGrQ9YgbxHQqDRXJ7AoCWJPnZt1U8Lb43Y5K0jTtPi+8ueJ/ukh5j9vTgoPyuZQN2dX8c2E41h/78fbArbXZpG+PlLklLj/guSc1fZwVtH5+zDC998x+B7dQa+VIGqX8Wn2/jt99Lj+F33xPW677Qn+EH7zCyn79XtVUOWzfiryR2+/0/uObm5qDylpaWkFEHEY1tIx5hZGVlwW63o7q6GvPmzQMAeL1e1NTUYNeuXSPu4COR3KeW3ucGoI0wivvjFWKx5IMgLl7ez3AjiMFljd3ikdDtW/LJbbRueU6JiMEqn3kmIVGcnzBjkvhT16DJP1U9/eKRkKE99L/D4LIJbfLvPcYOSZ6Fwt+edIKcEY4gRkIpYHg8HtTX3x/y+f1+3Lx5E5cvX0ZKSgoyMjJQUlKCffv2weFwIDs7G3v37kViYiLWrl07qp0nosdLKWB8/fXXePHFFwPbZWVlKCsrw/r161FRUYG33noL3d3deOedd+B2u1FYWAiXy4XkZPFygkQ0tigFjMWLF8Ptdg9br2kanE4nnE5npPpFRDEo+j+6IKIxgwGDiJQxYBCRMgYMIlI2PufDkN2njsRCMJI2dOPwWa4P9CeI61OT5JmefoS2M7isvUec1apJMiwBAEbxa01JFf8qucB2S9pEcpxX3g+BOE2SNwOgsUOckxLXEXouB5fFdcrbQJ88z2Is4wiDiJQxYBCRMgYMIlLGgEFEyhgwiEgZAwYRKWPAICJlDBhEpGx8Jm5JSKc4A6SLIenmeGF9X6I8FvsSxAlRiXHyiWf6/EMmtzGEKROwWsVTygHyyWuet/1FWF8w4bq0jQ5/6PIVg33fJ57o5789M6Vt3G0TT7s4sTk0ccsyqEzrV0hyE0xLCQAwyt8bTbKP7lf4+x0lHGEQkTIGDCJSxoBBRMoYMIhIGQMGESljwCAiZQwYRKTsiczDUKEZxbHUlyBeNs+vcGb9JnEeRptXnJsAAIlxQxbOMQFd/fIl/QK7G+W5BY6kZmF9rlk8QU6yQZ5Pcscnntzmrk+cQ+HT5RMW+XskORDhTsWgMr9Z4fPVJGlDsownIP/b0/slr3UUFzriCIOIlDFgEJEyBgwiUsaAQUTKGDCISBkDBhEpY8AgImVPZB6GZpDfs5fNW6BL7pUbe+T3wk2d4jbuNItzEwCgr3/IMdKBv7elBja7vOJ5O6ZM8kjbMBnEuRqdfrOwXjbXBQDU9diF9UaIz+df7qZL29C8kvc0zP+GwWX9Fvnnq98izoExSv5uAEheKUY1z0JGaYRx/vx5rFu3Dnl5ebBarTh69GhQfUlJCaxWa9C/H//4x6PSYSKKHqURRmdnJ/Lz87F+/Xr84he/CLvPc889h0OHDgW24+PFn2xENPYoBYzly5dj+fLlAIBf/vKXYfcxm82w28XDSiIa2yJ20bOmpgbZ2dkoLCzEm2++iTt37kTq0EQUIzS32/1QV1CmT5+ODz74AK+++mqg7Pjx47BYLMjKykJDQwPef/99+P1+nD17FmZz+AtidXV1I+s5EY0Kh8MxbF1E7pK89NJLgcezZ89GQUEB5syZg9OnT2PVqlVhn+NcUB54XHaxNGh7tGkm+cs22qcK6/sybcL6zhkTpG20PiW+at+T2SOsBwDr5OBZv/8tvRibGv89sB2JuyT/aK8X1j+T9DdhvV9hIDvSuySumwXSNm59K37PJv4t+P04/M+L8dqRLwPbSbfks3UnfSeehd14Uz7y9rvFs7T7vV7pMUaiqq1y2LpRycNIT0/HtGnTUF8v/kMjorFlVAJGa2srGhsbeRGUaJxR+kri8XgCowW/34+bN2/i8uXLSElJQUpKCvbs2YNVq1bBbrejoaEBu3btgs1mwwsvvDCqnX9kkkWKAEgnQtGN4uQvg8JaM3GSNYR6PfK3p91tDS5IB9rrB8p0o3go36rJL2FdS5wirE82iofIt3qswnoAMEi+cljjuoT1bZ3y5DCTR/y+G3pD+zC4zND3eBKm9CgmZskojTC+/vprLFmyBEuWLEF3dzfKysqwZMkS7N69G0ajEVeuXMHPf/5zzJ8/HyUlJcjOzsZnn32G5OTk0e4/ET1GSiOMxYsXw+12D1vvcrki1R8iimH88RkRKWPAICJlDBhEpIwBg4iUMWAQkbLxOYGOJs6R0OLlC/3oceJTo5vEsbbPorCwjmRNG1OnPJ7Hu0PbsTQNPK8/QXxPv8ciT2H/u2WysN7rE5/PeEO/tI0EU5+wvrknSVjf1ydfIEi61lG4+kFl4fI0hjJ0i1+H7pMvHCUl+fvmQkZEFBMYMIhIGQMGESljwCAiZQwYRKSMAYOIlDFgEJGy8ZmHIaGpLIEgydXoSxKfuv4EeR5GuIVzBtPEt/QBAMYws/gNLvPHSXJSeuSfGR0e8VwTyeZeYf2PptyWtuGXJElcbU8T1hsl834AQN8IPx4Vpg4B/JI8C10hD8P/GObDkOVyDIMjDCJSxoBBRMoYMIhIGQMGESljwCAiZQwYRKSMAYOIlDFgEJGy8Zm4JVuoSGFtVb9ZlrglbkNXCMW+CeIEHb/Cu9OXKC7zx4nb0C3yFZempnQI659KaRTW51rE9QBg1MQJTSaDuL7+Tqq8DXF+GcLN8zO4zNCnkHQlmyAnhhcpUsERBhEpY8AgImUMGESkjAGDiJQxYBCRMgYMIlLGgEFEysZlHoYmWYRIZSGjkYbSfpWFjCQ5Ev02SeIAgP6U0I52Zw6aecckbiMtvU3axnxbg7BelmfhMMsn0HH7EoT1Hf3iBZd62s3SNpLvit+ThDuhiRgJdwbyVEwd8vdD65UsZNQnX9RJaZKdkXrEfBDpf4v9+/dj6dKlyMjIwKxZs/DKK6/gypUrQ9rWUVZWhtzcXKSlpWHlypW4evXqI3WIiGKXNGCcO3cOmzZtwunTp3HixAmYTCasXr0abW0Dn0wHDhzAwYMHUV5ejjNnzsBms2HNmjXo6BBnCBLR2CL9SuJyuYK2Dx06hMzMTFy4cAErVqyAruuoqKjA1q1bUVxcDACoqKiAw+FAVVUVNm7cODo9J6LH7qG/qXs8Hvj9flitVgDA9evX0dTUhKKiosA+FosFixYtQm1tbcQ6SkTRp7nd7oe6+rFhwwZcu3YNZ8+ehdFoRG1tLZ5//nl88803yMjICOy3ZcsWNDY2hoxQHqirqxtZz4loVDgcjmHrHuouyY4dO3DhwgWcOnUKRqMxqE4bMm25rushZYM5F5QHHpddLA3aHinNLL5ibpwi/2WjLy1FWN/uSBbWd6bJB29d00Z+l2To3PnHC4rw0qUzAwURuEuycOp1Yb3sLkn+hO+lbcjukpxyzxXW//F/n5K2kXxVvLyE9VrwHYzf7FiKrburA9uWW93SNkzN94T1+l239Bj+ri7xMXySXxiP8BexVW2Vw9YpfyVxOp04fvw4Tpw4gZkzZwbK7XY7AKC5uTlo/5aWFthstofsKhHFMqURRmlpKVwuF06ePImcnJyguqysLNjtdlRXV2PevHkAAK/Xi5qaGuzatSvyPY4R8e3iKO9NUVjIyCD+JEiyyj/Rpk1sDynLmTXwie+YeEf4/CxLi7SNhQnXhPXTjR5hfYdfnvdypW+6uL5NvJCR+Za8jYTb4vwGc0voiG5wmfGu+HUCgH5PfGfQ3xNm5amhx3gcCxk9ImnA2LZtG44dO4YjR47AarWiqakJAJCYmIikpCRomoaSkhLs27cPDocD2dnZ2Lt3LxITE7F27dpRfwFE9PhIA0Zl5f3vMw9umT5QWloKp9MJAHjrrbfQ3d2Nd955B263G4WFhXC5XEhOFn/PJ6KxRRow3G639CCapsHpdAYCCBGNT/zxGREpY8AgImUMGESkjAGDiJQxYBCRsnE5gY5skhK9S54QZWgXT9gSZxEnCiXcUUgNTzMK63t75W/PP0z5e3BBT3DZzyb9l/D5kwzyhYySDeJ+3uoXv9Y/dIjTugHgi5ZsYf31b6cK69P+Kk92mlgvTrk2toQmXcXdHpTq3SZO+wYU0rpVJtDxy9+TaOEIg4iUMWAQkTIGDCJSxoBBRMoYMIhIGQMGESljwCAiZeMyD0N2H9t3L3TSmaEMveLp8eK94npj5yRpG7pB/PP/ViRJj3EyYXbQdrEdONkwUJbyw07h8zPi7krbqO8Vz5xWc/eHwvo/30qXtuG7LZ6ib+r/iCcksv7FLW3D0CyejlDvDpOf0zJwfvzdXmkbuuTvZqTT50UbRxhEpIwBg4iUMWAQkTIGDCJSxoBBRMoYMIhIGQMGESljwCAiZeMzcUtGF6+ABShMhNLbJ6w3eMQJUwAwqcMqrJ/QMll6jLbbQ9aJ3Qjofxgoq5i2Qvh8v3i5UQCApVmcNGVuEycjTe6RJytNcIuT7Sw3xauOabfkK7g9ynvq7xxI5tL7xe/5/Z3GdmKWDEcYRKSMAYOIlDFgEJEyBgwiUsaAQUTKGDCISBkDBhEpk+Zh7N+/H59++im+/fZbxMfHY/78+di5cyfy8/MD+5SUlOB3v/td0PPmz5+PP/3pT5HvcSRE4F657J680iQ9PT3CerPCwjnp9UMm2dkIpP/x+8CmP0k8MQ2M4hwLADB0SBZ+8kvyWlQW79HE/dC94slrdIW8F90n7me49zSobJznWKiQBoxz585h06ZNmDdvHnRdx+7du7F69WrU1tYiJSUlsN9zzz2HQ4cOBbbj4xUygohoTJEGDJfLFbR96NAhZGZm4sKFC1ixYiCL0Gw2w263R76HRBQzHvoahsfjgd/vh9VqDSqvqalBdnY2CgsL8eabb+LOnTuR6iMRxQjN7XY/1BezDRs24Nq1azh79iyMxvuL9B4/fhwWiwVZWVloaGjA+++/D7/fj7Nnz8JsNoc9Tl1d3ch7T0QR53A4hq17qICxY8cOuFwunDp1CjNnzhx2v8bGRsyZMweHDx/GqlWrwu6zNmVz4HHZxVI4F5SrdiM2SC7SQZMP3gwTwgfTwCEs4hXkAUBLDr7o+d4n6/GvPxu4AM2LnoP2eciLnmW1/wLnwg8G7fBkXPSsaqsctk7516pOpxMulwuffvqpMFgAQHp6OqZNm4b6+nrlThJR7FMKGKWlpXC5XDh58iRycnKk+7e2tqKxsZEXQYnGGWnA2LZtG44dO4YjR47AarWiqakJAJCYmIikpCR4PB7s2bMHq1atgt1uR0NDA3bt2gWbzYYXXnhh1F9A1EiHpwpzbnjFeRiQ1QPQwgzFfY1NA/Wy29s+8TwUAOCXfOXQZedCoQ3NJP5TlLWhq3ztkc2DEq6NJ+RriCppwKisvP99pri4OKi8tLQUTqcTRqMRV65cwe9//3vcu3cPdrsdixcvxm9/+1skJ4tX9iKisUUaMNxut7DeYrGE5GoQ0fjE35IQkTIGDCJSxoBBRMoYMIhIGQMGESljwCAiZU/mQkaPg0rCjy5PaJIeoif0GPqgiXl0ySQ9sUKXJXcxgSomcIRBRMoYMIhIGQMGESljwCAiZQwYRKSMAYOIlD30nJ5E9OTiCIOIlDFgEJEyBgwiUsaAQUTKGDCISFlUA0ZlZSXmzp0Lu92OZ599Fl999VU0uzNmnD9/HuvWrUNeXh6sViuOHj0aVK/rOsrKypCbm4u0tDSsXLkSV69ejVJvY9f+/fuxdOlSZGRkYNasWXjllVdw5cqVoH14LoNFLWC4XC5s374db7/9Nr744gssWLAAL7/8Mm7cuBGtLo0ZnZ2dyM/Px549e2CxWELqDxw4gIMHD6K8vBxnzpyBzWbDmjVr0NHREYXexq5z585h06ZNOH36NE6cOAGTyYTVq1ejra0tsA/PZbCo5WEsW7YMs2fPxkcffRQomzdvHoqLi7Fz585odGlMmj59Oj744AO8+uqrAO5/Iubm5uL111/Htm3bAADd3d1wOBx47733sHHjxmh2N6Z5PB5kZmbi6NGjWLFiBc9lGFEZYfT29uLSpUsoKioKKi8qKkJtbW00ujRuXL9+HU1NTUHn1mKxYNGiRTy3Eh6PB36/H1arFQDPZThRCRitra3w+Xyw2WxB5TabDc3NzdHo0rjxYGU6ntuHt337dsyZMwcLFiwAwHMZTlRn3NKGrNit63pIGT0antuHs2PHDly4cAGnTp2C0WgMquO5HBCVEUZqaiqMRmNIlG5paQmJ5vRwHiyAzXOrzul04vjx4zhx4gRmzpwZKOe5DBWVgBEfH4+CggJUV1cHlVdXV2PhwoXR6NK4kZWVBbvdHnRuvV4vampqeG7DKC0tRVVVFU6cOIGcnJygOp7LUFH7SrJlyxa88cYbKCwsxMKFC3H48GHcvn37ibzy/LA8Hg/q6+sB3F9Z/ebNm7h8+TJSUlKQkZGBkpIS7Nu3Dw6HA9nZ2di7dy8SExOxdu3aKPc8tmzbtg3Hjh3DkSNHYLVaA9csEhMTkZSUBE3TeC6HiOrP2ysrK3HgwAE0NTUhLy8Pu3fvxjPPPBOt7owZX375JV588cWQ8vXr16OiogK6rmPPnj34+OOP4Xa7UVhYiL179yI/Pz8KvY1dD+6GDFVaWgqn0wkAPJdDcD4MIlLG35IQkTIGDCJSxoBBRMoYMIhIGQMGESljwCAiZQwYRKSMAYOIlDFgEJGy/weuhVod9xn9SQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "fig = plt.figure(figsize=(20., 5.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(2, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    image,_ = dataset.__getitem__(random.randint(1,37000))\n",
    "\n",
    "    img = image.reshape(-1, 28*28)\n",
    "    _,reconstructed = model(img)\n",
    "    item = reconstructed.reshape(-1, 28, 28)\n",
    "    item = item.detach().numpy()\n",
    "\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    grid[i].imshow(image[0])\n",
    "    grid[i].grid(False)\n",
    "    grid[10+i].imshow(item[0])\n",
    "    grid[10+i].grid(False)\n",
    "\n",
    "\n",
    "plt.savefig(\"cae_nums.png\",transparent=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
