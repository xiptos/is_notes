{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiptos/is_notes/blob/main/cnn_fmnist.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook presents a simple guide to creating a convolutional neural network with PyTorch. It will predict the outcome of the fashion images from the [Zalando's article images](https://github.com/zalandoresearch/fashion-mnist) (Fashion-MNIST).\n",
    "\n",
    "The guide contains the most elementary PyTorch elements to create and evaluate a network.\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader # loads data in batches\n",
    "from torchvision import datasets # load Fasion-MNIST\n",
    "import torchvision.transforms as T # transformers for computer vision "
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-11-20T11:59:55.135283Z",
     "iopub.execute_input": "2022-11-20T11:59:55.135779Z",
     "iopub.status.idle": "2022-11-20T11:59:57.228738Z",
     "shell.execute_reply.started": "2022-11-20T11:59:55.135682Z",
     "shell.execute_reply": "2022-11-20T11:59:57.227699Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-20T12:00:09.84164Z",
     "iopub.execute_input": "2022-11-20T12:00:09.842235Z",
     "iopub.status.idle": "2022-11-20T12:00:09.84754Z",
     "shell.execute_reply.started": "2022-11-20T12:00:09.842194Z",
     "shell.execute_reply": "2022-11-20T12:00:09.846605Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Torchvision datasets\n",
    "\n",
    "We obtain the [Fashion-MNIST dataset](http://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) via torchvision. The dataset contains a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Note that `datasets` is an object imported from torchvision, not to confuse with from the Dataset object (used in torch.utils.data import Dataset)\n",
    "\n",
    "```from torchvision import datasets```\n",
    "\n",
    "When called for the first time, the datasets will be downloaded to the path specified in the `root` argument. After that, Torchvision will look first for a local copy before attempting another download."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-05T23:37:48.188818Z",
     "iopub.execute_input": "2022-08-05T23:37:48.18922Z",
     "iopub.status.idle": "2022-08-05T23:37:48.193684Z",
     "shell.execute_reply.started": "2022-08-05T23:37:48.18919Z",
     "shell.execute_reply": "2022-08-05T23:37:48.192628Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **torchvision.transforms**. A transformer operates on the data. Using the ' transform' argument, we can apply multiple transformations (reshape, convert to tensor, normalize, etc.) to the data obtained."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-06T22:12:57.413031Z",
     "iopub.execute_input": "2022-08-06T22:12:57.413408Z",
     "iopub.status.idle": "2022-08-06T22:12:57.419602Z",
     "shell.execute_reply.started": "2022-08-06T22:12:57.413377Z",
     "shell.execute_reply": "2022-08-06T22:12:57.418347Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mytransform = T.ToTensor() # image (3D array) to Tensor\n",
    "\n",
    "train_data = datasets.FashionMNIST(root = './', download=True, train = True, transform = mytransform)\n",
    "test_data = datasets.FashionMNIST(root = './', download=True, train = False, transform = mytransform)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-20T12:00:16.383148Z",
     "iopub.execute_input": "2022-11-20T12:00:16.3836Z",
     "iopub.status.idle": "2022-11-20T12:00:19.65425Z",
     "shell.execute_reply.started": "2022-11-20T12:00:16.383563Z",
     "shell.execute_reply": "2022-11-20T12:00:19.653126Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the first image in the dataset is a 3D tensor (C, H, W) for the number of channels (C), Height (H), and Width (W)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "image, label = next(iter(train_data))\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "print(label)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-20T12:00:51.706437Z",
     "iopub.execute_input": "2022-11-20T12:00:51.70682Z",
     "iopub.status.idle": "2022-11-20T12:00:51.736293Z",
     "shell.execute_reply.started": "2022-11-20T12:00:51.706789Z",
     "shell.execute_reply": "2022-11-20T12:00:51.735106Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img, label = train_data[0]\n",
    "img.shape # returns a Tensor of Size 1,28,28"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DataLoader\n",
    "\n",
    "The PyTorch DataLoader object allows the preparation of the dataset in batches of different sizes and shuffles them if necessary when exposing them to the training. \n",
    "\n",
    "```from torch.utils.data import DataLoader```\n",
    "\n",
    "> Note that the DataLoader object shuffles the data by default."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## We have 10 classes, as follows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "                 0: \"T-shirt/Top\",\n",
    "                 1: \"Trouser\",\n",
    "                 2: \"Pullover\",\n",
    "                 3: \"Dress\",\n",
    "                 4: \"Coat\", \n",
    "                 5: \"Sandal\", \n",
    "                 6: \"Shirt\",\n",
    "                 7: \"Sneaker\",\n",
    "                 8: \"Bag\",\n",
    "                 9: \"Ankle Boot\"\n",
    "                 }\n",
    "    input = (label.item() if type(label) == torch.Tensor else label)\n",
    "    return output_mapping[input]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "demo_loader = torch.utils.data.DataLoader(train_data, batch_size=10)\n",
    "\n",
    "batch = next(iter(demo_loader))\n",
    "images, labels = batch\n",
    "print(type(images), type(labels))\n",
    "print(images.shape, labels.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use torchvision functionality to visualize a batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "grid = torchvision.utils.make_grid(images, nrow=10)\n",
    "\n",
    "plt.figure(figsize=(15, 20))\n",
    "plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "print(\"labels: \", end=\" \")\n",
    "for i, l in enumerate(labels):\n",
    "    print(output_label(l), end=\", \")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(101)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = 100, shuffle=True)\n",
    "# the test loader can be bigger and doesn't need to be shuffled\n",
    "test_loader =  DataLoader(test_data,  batch_size = 100, shuffle=False) "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the network\n",
    "\n",
    "*   Make a model class (FashionCNN in our case)\n",
    "    * It inherits nn.Module class that is a super class for all the neural networks in Pytorch.\n",
    "*   The Neural Net should have the following layers:\n",
    "    * Two Sequential layers each consists of following layers-\n",
    "        * Convolution layer that has kernel size of 3 * 3, padding = 1 (zero_padding) in 1st layer and padding = 0 in second one. Stride of 1 in both layer.\n",
    "        * Batch Normalization layer.\n",
    "        * Acitvation function: ReLU.\n",
    "        * Max Pooling layer with kernel size of 2 * 2 and stride 2.\n",
    "     * Flatten out the output for dense layer(a.k.a. fully connected layer).\n",
    "     * 3 Fully connected layer  with different in/out features.\n",
    "     * 1 Dropout layer that has class probability p = 0.25.\n",
    "  \n",
    "     * All the functionaltiy is given in forward method that defines the forward pass of CNN.\n",
    "     * Our input image is changing in a following way:\n",
    "        * First Convulation layer : input: 28 \\* 28 \\* 3, output: 28 \\* 28 \\* 32\n",
    "        * First Max Pooling layer : input: 28 \\* 28 \\* 32, output: 14 \\* 14 \\* 32\n",
    "        * Second Conv layer : input : 14 \\* 14 \\* 32, output: 12 \\* 12 \\* 64\n",
    "        * Second Max Pooling layer : 12 \\* 12 \\* 64, output:  6 \\* 6 \\* 64\n",
    "    * Final fully connected layer has 10 output features for 10 types of clothes.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FashionCNN, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-20T12:01:06.042304Z",
     "iopub.execute_input": "2022-11-20T12:01:06.042691Z",
     "iopub.status.idle": "2022-11-20T12:01:06.05135Z",
     "shell.execute_reply.started": "2022-11-20T12:01:06.042661Z",
     "shell.execute_reply": "2022-11-20T12:01:06.050039Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(101)\n",
    "\n",
    "model = FashionCNN()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-20T12:01:07.534523Z",
     "iopub.execute_input": "2022-11-20T12:01:07.535632Z",
     "iopub.status.idle": "2022-11-20T12:01:07.54395Z",
     "shell.execute_reply.started": "2022-11-20T12:01:07.535591Z",
     "shell.execute_reply": "2022-11-20T12:01:07.542867Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We select the [cross-entroy](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html?highlight=entropy) as the cost function. The cross-entropy is similar to the quadratic formula, but it predicts the probability distribution of each class.\n",
    "\n",
    "* We define the optimization method. The simplest one is the [Adaptative Stochastic Gradient Descent method](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "learning_rate = 1e-3\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-20T12:01:10.616172Z",
     "iopub.execute_input": "2022-11-20T12:01:10.617368Z",
     "iopub.status.idle": "2022-11-20T12:01:10.622927Z",
     "shell.execute_reply.started": "2022-11-20T12:01:10.617323Z",
     "shell.execute_reply": "2022-11-20T12:01:10.621805Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training and evaluation \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Train for 10 epocs\n",
    "num_epochs = 5\n",
    "count = 0\n",
    "# Lists for visualization of loss and accuracy \n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "# Lists for knowing classwise accuracy\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        # Transfering images and labels to GPU if available\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "        train = Variable(images.view(100, 1, 28, 28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward pass \n",
    "        \n",
    "        # Initializing a gradient as 0 so there is no mixing of gradient among the batches\n",
    "        \n",
    "        #Propagating the error backward\n",
    "        \n",
    "        # Optimizing the parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "        count += 1\n",
    "    \n",
    "    # Testing the model\n",
    "    \n",
    "        if not (count % 50):    # It's same as \"if count % 50 == 0\"\n",
    "            total = 0\n",
    "            correct = 0\n",
    "        \n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                labels_list.append(labels)\n",
    "            \n",
    "                test = Variable(images.view(100, 1, 28, 28))\n",
    "            \n",
    "                outputs = model(test)\n",
    "            \n",
    "                predictions = torch.max(outputs, 1)[1].to(device)\n",
    "                predictions_list.append(predictions)\n",
    "                correct += (predictions == labels).sum()\n",
    "            \n",
    "                total += len(labels)\n",
    "            \n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        \n",
    "        if not (count % 500):\n",
    "            print(\"Iteration: {}, Loss: {}, Accuracy: {}%\".format(count, loss.data, accuracy))\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-20T12:03:24.595312Z",
     "iopub.execute_input": "2022-11-20T12:03:24.596362Z",
     "iopub.status.idle": "2022-11-20T12:04:51.579405Z",
     "shell.execute_reply.started": "2022-11-20T12:03:24.596303Z",
     "shell.execute_reply": "2022-11-20T12:04:51.578093Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-06T02:24:07.967843Z",
     "iopub.execute_input": "2022-08-06T02:24:07.968441Z",
     "iopub.status.idle": "2022-08-06T02:24:07.972846Z",
     "shell.execute_reply.started": "2022-08-06T02:24:07.968408Z",
     "shell.execute_reply": "2022-08-06T02:24:07.971569Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll see train and test losses, together with its accuracies per epoch. Note that the training data have more minor losses and reach an accuracy of almost 100%. On the other hand, the test data reach almost a plateau of > 95% accuracy, and we could think of using more than two epochs because this is where the training data crosses the accuracy of the testing data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(iteration_list, loss_list)\n",
    "plt.xlabel(\"No. of Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Iterations vs Loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-20T12:19:02.347292Z",
     "iopub.execute_input": "2022-11-20T12:19:02.347668Z",
     "iopub.status.idle": "2022-11-20T12:19:02.694434Z",
     "shell.execute_reply.started": "2022-11-20T12:19:02.347638Z",
     "shell.execute_reply": "2022-11-20T12:19:02.693078Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we evaluate all the test data at once and visualize the accuracy of every outcome for every prediction (i.e., confusion matrix)."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-08T00:59:08.129764Z",
     "iopub.execute_input": "2022-08-08T00:59:08.130182Z",
     "iopub.status.idle": "2022-08-08T00:59:08.137731Z",
     "shell.execute_reply.started": "2022-08-08T00:59:08.130148Z",
     "shell.execute_reply": "2022-08-08T00:59:08.136336Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(iteration_list, accuracy_list)\n",
    "plt.xlabel(\"No. of Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Iterations vs Accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-20T12:19:10.729477Z",
     "iopub.execute_input": "2022-11-20T12:19:10.729899Z",
     "iopub.status.idle": "2022-11-20T12:19:11.901358Z",
     "shell.execute_reply.started": "2022-11-20T12:19:10.729859Z",
     "shell.execute_reply": "2022-11-20T12:19:11.900128Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class_correct = [0. for _ in range(10)]\n",
    "total_correct = [0. for _ in range(10)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        test = Variable(images)\n",
    "        outputs = model(test)\n",
    "        predicted = torch.max(outputs, 1)[1]\n",
    "        c = (predicted == labels).squeeze()\n",
    "        \n",
    "        for i in range(100):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            total_correct[label] += 1\n",
    "        \n",
    "for i in range(10):\n",
    "    print(\"Accuracy of {}: {:.2f}%\".format(output_label(i), class_correct[i] * 100 / total_correct[i]))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-20T12:19:15.641037Z",
     "iopub.execute_input": "2022-11-20T12:19:15.641444Z",
     "iopub.status.idle": "2022-11-20T12:19:16.637063Z",
     "shell.execute_reply.started": "2022-11-20T12:19:15.641413Z",
     "shell.execute_reply": "2022-11-20T12:19:16.635805Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import chain \n",
    "\n",
    "predictions_l = [predictions_list[i].tolist() for i in range(len(predictions_list))]\n",
    "labels_l = [labels_list[i].tolist() for i in range(len(labels_list))]\n",
    "predictions_l = list(chain.from_iterable(predictions_l))\n",
    "labels_l = list(chain.from_iterable(labels_l))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Show the confusion matrix\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "confusion_matrix(labels_l, predictions_l)\n",
    "# print(\"Classification report for CNN :\\n%s\\n\"\n",
    "#       % (metrics.classification_report(labels_l, predictions_l)))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-20T12:19:19.35135Z",
     "iopub.execute_input": "2022-11-20T12:19:19.351999Z",
     "iopub.status.idle": "2022-11-20T12:19:19.802896Z",
     "shell.execute_reply.started": "2022-11-20T12:19:19.351962Z",
     "shell.execute_reply": "2022-11-20T12:19:19.801705Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Show the heatmap corresponding to the confusion matrix"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-20T12:19:22.262835Z",
     "iopub.execute_input": "2022-11-20T12:19:22.263252Z",
     "iopub.status.idle": "2022-11-20T12:19:23.045818Z",
     "shell.execute_reply.started": "2022-11-20T12:19:22.263218Z",
     "shell.execute_reply": "2022-11-20T12:19:23.04464Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
