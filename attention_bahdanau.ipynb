{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiptos/is_notes/blob/main/attention_bahdanau.ipynb)",
   "id": "411b2a0920d8281c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Introduction to attention mechanism",
   "id": "e24bb49d889f1c24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Attention demo: classic (Bahdanau and Luong) on toy vectors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "\n",
    "# Toy encoder hidden states: 3 time steps, dim = 4\n",
    "# Shape: (T, d_h) = (3, 4)\n",
    "encoder_states = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0],  # h1\n",
    "    [0.0, 1.0, 0.0, 0.0],  # h2\n",
    "    [0.0, 0.0, 1.0, 0.0]   # h3\n",
    "])\n",
    "\n",
    "# Toy decoder state: dim = 4\n",
    "decoder_state = torch.tensor([0.5, 1.0, 0.5, 0.0])  # s_t"
   ],
   "id": "b9e88d1991270095",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bahdanau (additive) attention (small dimensions)",
   "id": "dce9fbd27b41c61b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, attn_dim):\n",
    "        super().__init__()\n",
    "        self.W_s = nn.Linear(hidden_dim, attn_dim, bias=False)\n",
    "        self.W_h = nn.Linear(hidden_dim, attn_dim, bias=False)\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, decoder_state, encoder_states):\n",
    "        \"\"\"\n",
    "        decoder_state: (hidden_dim,)\n",
    "        encoder_states: (T, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Add batch dimension: (1, hidden_dim)\n",
    "        s = decoder_state.unsqueeze(0)       # (1, d)\n",
    "        h = encoder_states                   # (T, d)\n",
    "\n",
    "        # Expand s over time steps\n",
    "        # s_expanded: (T, d)\n",
    "        s_expanded = s.expand(h.size(0), -1)\n",
    "\n",
    "        # Compute energies e_{t,i}\n",
    "        # W_s s + W_h h -> (T, attn_dim)\n",
    "        energy = torch.tanh(self.W_s(s_expanded) + self.W_h(h))\n",
    "        # v^T * energy -> (T, 1) -> (T,)\n",
    "        scores = self.v(energy).squeeze(-1)\n",
    "\n",
    "        # Attention weights (softmax over time dimension)\n",
    "        attn_weights = F.softmax(scores, dim=0)  # (T,)\n",
    "\n",
    "        # Context vector: weighted sum of encoder states\n",
    "        # (T, d) * (T,) -> (d,)\n",
    "        context = torch.sum(encoder_states * attn_weights.unsqueeze(-1), dim=0)\n",
    "\n",
    "        return context, attn_weights, scores\n",
    "\n",
    "bahdanau = BahdanauAttention(hidden_dim=4, attn_dim=3)\n",
    "\n",
    "context_b, alpha_b, scores_b = bahdanau(decoder_state, encoder_states)\n",
    "\n",
    "print(\"Bahdanau scores:\", scores_b)\n",
    "print(\"Bahdanau attention weights:\", alpha_b)\n",
    "print(\"Bahdanau context vector:\", context_b)"
   ],
   "id": "a6f409409f61d02c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Luong (dot-product) attention",
   "id": "b77d488445528291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def luong_dot_attention(decoder_state, encoder_states):\n",
    "    \"\"\"\n",
    "    Simple dot-product attention.\n",
    "    decoder_state: (d,)\n",
    "    encoder_states: (T, d)\n",
    "    \"\"\"\n",
    "    # scores: (T,)\n",
    "    scores = torch.mv(encoder_states, decoder_state)  # matrix-vector product\n",
    "    attn_weights = F.softmax(scores, dim=0)           # (T,)\n",
    "    context = torch.sum(encoder_states * attn_weights.unsqueeze(-1), dim=0)\n",
    "    return context, attn_weights, scores\n",
    "\n",
    "context_l, alpha_l, scores_l = luong_dot_attention(decoder_state, encoder_states)\n",
    "\n",
    "print(\"Luong scores:\", scores_l)\n",
    "print(\"Luong attention weights:\", alpha_l)\n",
    "print(\"Luong context vector:\", context_l)"
   ],
   "id": "c1cba575a2ac84c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visual comparison and interpretation",
   "id": "e88dbf27790fa99d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"=== Bahdanau vs Luong ===\")\n",
    "print(\"Bahdanau weights:\", alpha_b)\n",
    "print(\"Luong weights    :\", alpha_l)\n",
    "\n",
    "print(\"\\nDo both mechanisms focus on similar time steps?\")\n",
    "print(\"Sum of absolute differences:\", torch.sum(torch.abs(alpha_b - alpha_l)).item())"
   ],
   "id": "8031463afaf6415f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot attention weights",
   "id": "d54c266eb52e1b55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time_steps = range(1, encoder_states.size(0) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.stem(time_steps, alpha_b.detach().numpy(), markerfmt='o')\n",
    "plt.xlabel(\"Encoder time step\")\n",
    "plt.ylabel(\"Bahdanau weight\")\n",
    "plt.title(\"Bahdanau Attention Weights\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.stem(time_steps, alpha_l.detach().numpy(), markerfmt='o')\n",
    "plt.xlabel(\"Encoder time step\")\n",
    "plt.ylabel(\"Luong weight\")\n",
    "plt.title(\"Luong Attention Weights\")\n",
    "plt.show()"
   ],
   "id": "2fd3d9fcd4da339a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention_heatmap(attention_weights, title=\"Attention Heatmap\"):\n",
    "    \"\"\"\n",
    "    attention_weights: 1D tensor or list, shape (T,)\n",
    "    \"\"\"\n",
    "    weights = attention_weights.detach().numpy()\n",
    "    weights = weights.reshape(1, -1)  # shape (1, T)\n",
    "\n",
    "    plt.figure(figsize=(6, 2))\n",
    "    plt.imshow(weights, cmap=\"viridis\", aspect=\"auto\")\n",
    "\n",
    "    plt.colorbar(label=\"Attention weight\")\n",
    "    plt.yticks([])  # hide y-axis since it's a single decoder step\n",
    "    plt.xticks(\n",
    "        ticks=np.arange(len(weights[0])),\n",
    "        labels=[f\"t={i+1}\" for i in range(len(weights[0]))]\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Encoder time step\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_attention_heatmap(alpha_b, \"Bahdanau Attention Heatmap\")\n",
    "plot_attention_heatmap(alpha_l, \"Luong Attention Heatmap\")"
   ],
   "id": "f774ff726e4ae20c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attention by hand",
   "id": "e26e935b01f59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise A – Softmax and Attention Weights\n",
    "\n",
    "**Given scores for one decoder step:**\n",
    "\n",
    "$$\n",
    "e_{t,1} = 0,\\quad e_{t,2} = 1,\\quad e_{t,3} = 2\n",
    "$$\n",
    "\n",
    "1. Compute the attention weights:\n",
    "   $$\n",
    "   \\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^3 \\exp(e_{t,j})}, \\quad i = 1,2,3.\n",
    "   $$\n",
    "\n",
    "2. Verify that:\n",
    "   $$\n",
    "   \\alpha_{t,1} + \\alpha_{t,2} + \\alpha_{t,3} = 1.\n",
    "   $$\n",
    "\n",
    "3. Which position has the highest attention?\n",
    "   Does that correspond to the largest score?\n",
    "\n",
    "> Hint: You may approximate \\( e \\approx 2.72 \\) if you compute by hand."
   ],
   "id": "e96e45ec3cad0a32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise B – Context Vector with 1D Encoder States\n",
    "\n",
    "Assume the encoder hidden states are **1-dimensional**:\n",
    "\n",
    "- \\( h_1 = 1 \\)\n",
    "- \\( h_2 = 2 \\)\n",
    "- \\( h_3 = 4 \\)\n",
    "\n",
    "Use the attention weights \\(\\alpha_{t,1}, \\alpha_{t,2}, \\alpha_{t,3}\\) obtained in **Exercise A**.\n",
    "\n",
    "1. Compute the context vector:\n",
    "   $$\n",
    "   c_t = \\alpha_{t,1} h_1 + \\alpha_{t,2} h_2 + \\alpha_{t,3} h_3.\n",
    "   $$\n",
    "\n",
    "2. Is \\(c_t\\) closer to 1, 2, or 4?\n",
    "   How does this relate to which \\(\\alpha_{t,i}\\) was largest?\n",
    "\n",
    "3. Intuitively, what does this say about which encoder position the model is “focusing” on?"
   ],
   "id": "f3efb701423072d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise C – Dot-Product Attention with Tiny Vectors\n",
    "\n",
    "Consider encoder states in 2D and one decoder state:\n",
    "\n",
    "- \\( h_1 = (1, 0) \\)\n",
    "- \\( h_2 = (0, 1) \\)\n",
    "- \\( h_3 = (1, 1) \\)\n",
    "- \\( s_t = (1, 1) \\)\n",
    "\n",
    "1. Compute the dot-product scores:\n",
    "   $$\n",
    "   e_{t,i} = s_t^\\top h_i, \\quad i = 1,2,3.\n",
    "   $$\n",
    "\n",
    "2. Apply the softmax to obtain attention weights:\n",
    "   $$\n",
    "   \\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^3 \\exp(e_{t,j})}.\n",
    "   $$\n",
    "\n",
    "3. Compute the **2D context vector**:\n",
    "   $$\n",
    "   c_t = \\sum_{i=1}^3 \\alpha_{t,i} h_i.\n",
    "   $$\n",
    "\n",
    "4. Interpret the result:\n",
    "   - Which encoder state does the decoder “care” most about?\n",
    "   - How can you see that both from the scores and from the context vector?"
   ],
   "id": "5190340af7c05c07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise D – Interpreting an Attention Matrix\n",
    "\n",
    "Suppose you have **2 output tokens** and **3 input tokens**, with attention weights:\n",
    "\n",
    "$$\n",
    "\\alpha =\n",
    "\\begin{bmatrix}\n",
    "0.7 & 0.2 & 0.1 \\\\\n",
    "0.1 & 0.3 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **Rows** correspond to decoder/output positions (row 1 = output 1, row 2 = output 2).\n",
    "- **Columns** correspond to encoder/input positions (column 1 = input 1, etc.).\n",
    "\n",
    "Answer the following:\n",
    "\n",
    "1. When predicting the **first output word** (row 1), which input word is most important? Why?\n",
    "\n",
    "2. When predicting the **second output word** (row 2), which input word is most important? Why?\n",
    "\n",
    "3. If this were a **machine translation** model:\n",
    "   - What kind of alignment between input and output words does this matrix suggest?\n",
    "   - Give a possible example of a short input and output sequence that could produce a pattern like this."
   ],
   "id": "107110acbd1db14e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "758f89764ac88348",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
