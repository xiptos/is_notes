{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiptos/is_notes/blob/main/rl_cartpole.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reinforcement learning problems involve learning what to do, namelly how to map situations to actions, so as to maximize a numerical reward signal.\n",
    "\n",
    "In this example, let's take a look at the CartPole example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium[classic-control]\r\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from gymnasium[classic-control]) (1.26.2)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from gymnasium[classic-control]) (2.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from gymnasium[classic-control]) (4.7.1)\r\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium[classic-control])\r\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\r\n",
      "Collecting pygame>=2.1.3 (from gymnasium[classic-control])\r\n",
      "  Downloading pygame-2.5.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\r\n",
      "Downloading pygame-2.5.2-cp310-cp310-macosx_11_0_arm64.whl (12.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.2/12.2 MB\u001B[0m \u001B[31m984.8 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m953.9/953.9 kB\u001B[0m \u001B[31m5.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: farama-notifications, pygame, gymnasium\r\n",
      "  Attempting uninstall: pygame\r\n",
      "    Found existing installation: pygame 2.1.0\r\n",
      "    Uninstalling pygame-2.1.0:\r\n",
      "      Successfully uninstalled pygame-2.1.0\r\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pygame-2.5.2\r\n",
      "Requirement already satisfied: moviepy in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (1.0.3)\r\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from moviepy) (4.4.2)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from moviepy) (4.65.0)\r\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from moviepy) (2.31.0)\r\n",
      "Requirement already satisfied: proglog<=1.0.0 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from moviepy) (0.1.10)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from moviepy) (1.26.2)\r\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from moviepy) (2.22.4)\r\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from moviepy) (0.4.7)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (10.0.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rlopes/miniconda3/envs/is/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2023.11.17)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"gymnasium[classic-control]\"\n",
    "!pip install moviepy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:01:06.308701Z",
     "start_time": "2024-01-12T17:00:46.425416Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T17:01:44.202791Z",
     "start_time": "2024-01-12T17:01:25.922161Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from gymnasium.utils.save_video import save_video\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Control options"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x16e835750>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma=0.99\n",
    "render=\"human\"\n",
    "#render=\"rgb_array_list\"\n",
    "log_interval=10\n",
    "seed=543\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode=render)\n",
    "env.reset(seed=seed)\n",
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:01:57.617565Z",
     "start_time": "2024-01-12T17:01:46.763215Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q(s,a) neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T17:01:57.623519Z",
     "start_time": "2024-01-12T17:01:57.619898Z"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:02:03.364884Z",
     "start_time": "2024-01-12T17:02:03.166599Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = deque()\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.appendleft(R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "def save_episode(episode: int):\n",
    "    return episode % 10 == 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:03:52.890296Z",
     "start_time": "2024-01-12T17:03:52.885324Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "50"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.metadata[\"render_fps\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:03:53.324280Z",
     "start_time": "2024-01-12T17:03:53.320473Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main cicle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T17:04:06.422681Z",
     "start_time": "2024-01-12T17:03:54.156373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-4.3390, grad_fn=<SumBackward0>)\n",
      "tensor(0.6294, grad_fn=<SumBackward0>)\n",
      "tensor(0.2726, grad_fn=<SumBackward0>)\n",
      "tensor(1.9164, grad_fn=<SumBackward0>)\n",
      "tensor(-0.5062, grad_fn=<SumBackward0>)\n",
      "tensor(-3.8794, grad_fn=<SumBackward0>)\n",
      "tensor(-4.7876, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m10000\u001B[39m):  \u001B[38;5;66;03m# Don't infinite loop while learning\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     action \u001B[38;5;241m=\u001B[39m select_action(state)\n\u001B[0;32m----> 8\u001B[0m     state, reward, done, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m render\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     10\u001B[0m         env\u001B[38;5;241m.\u001B[39mrender()\n",
      "File \u001B[0;32m~/miniconda3/envs/is/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     47\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     55\u001B[0m \n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 57\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[0;32m~/miniconda3/envs/is/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/is/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:51\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/is/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:190\u001B[0m, in \u001B[0;36mCartPoleEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    187\u001B[0m     reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 190\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32), reward, terminated, \u001B[38;5;28;01mFalse\u001B[39;00m, {}\n",
      "File \u001B[0;32m~/miniconda3/envs/is/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:302\u001B[0m, in \u001B[0;36mCartPoleEnv.render\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    301\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrender_fps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    303\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mflip()\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "running_reward = 10\n",
    "step_starting_index = 0\n",
    "for i_episode in count(1):\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        if render==\"human\":\n",
    "            env.render()\n",
    "        policy.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if render==\"rgb_array_list\":\n",
    "        save_video(env.render(),\n",
    "               \"videos\", fps=int(env.metadata[\"render_fps\"]), step_starting_index=step_starting_index,\n",
    "               episode_index=i_episode, episode_trigger=save_episode)\n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    finish_episode()\n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "            i_episode, ep_reward, running_reward))\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        torch.save(policy.state_dict(), 'cartpole_policy.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autonomous play..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils import play\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Allows for loading and running a pre-trained model,\n",
    "# or for the user to interact with the environment.\n",
    "# See 'cartpole_train' for more details.\n",
    "\n",
    "USER_PLAY = False\n",
    "MODEL_DIR = \"cartpole_policy.pth\"\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "if USER_PLAY:\n",
    "    env.reset()\n",
    "    key_action_map = {(ord('a'),): 0, (ord('d'),): 1}  # Map 'a' and 'd' to going left and right\n",
    "    play.play(env, keys_to_action=key_action_map, fps=15)  # Lower fps cause its too hard otherwise\n",
    "    sys.exit()\n",
    "\n",
    "# 0: Left, 1: Right\n",
    "action_size = env.action_space.n\n",
    "# 0: Cart position, 1: Cart Velocity, 2: Pole Angle, 3: Pole Angular Velocity\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "model = Policy()\n",
    "model.load_state_dict(torch.load(MODEL_DIR))\n",
    "model.eval()\n",
    "n_episodes = 40\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.asarray(state[0])\n",
    "\n",
    "    done = False\n",
    "    time_step = 0\n",
    "\n",
    "    score = 0\n",
    "    while not done and time_step < 500:\n",
    "        env.render()\n",
    "\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "        score += reward\n",
    "        reward = 0.1 if not done else -1\n",
    "        next_state = np.asarray(next_state)\n",
    "\n",
    "        state = next_state\n",
    "        time_step += 1\n",
    "\n",
    "    print(\"episode: {}, time steps: {}, score: {}\".format(e, time_step, score))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
