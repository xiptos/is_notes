{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiptos/is_notes/blob/main/rl_cartpole.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reinforcement learning problems involve learning what to do, namelly how to map situations to actions, so as to maximize a numerical reward signal.\n",
    "\n",
    "In this example, let's take a look at the CartPole example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install \"gymnasium[classic-control]\"\n",
    "!pip install moviepy"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from gymnasium.utils.save_video import save_video\n",
    "from torch.distributions import Categorical"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Control options"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "gamma=0.99\n",
    "# render=\"human\"\n",
    "render=\"rgb_array_list\"\n",
    "log_interval=10\n",
    "seed=543\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode=render)\n",
    "env.reset(seed=seed)\n",
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## $\\pi(s, a)$ neural network",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "# finish_episode() is called at the end of every episode to:\n",
    "#\t1.\tCompute the returns (discounted sum of rewards)\n",
    "#\t2.\tNormalize them (variance reduction)\n",
    "#\t3.\tCompute the policy gradient loss\n",
    "#\t4.\tPerform a backward pass and optimize the policy network\n",
    "#\t5.\tClear stored rewards and log-probs for the next episode\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = deque()\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.appendleft(R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "def save_episode(episode: int):\n",
    "    return episode % 10 == 0\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "env.metadata[\"render_fps\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main cicle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "running_reward = 10\n",
    "step_starting_index = 0\n",
    "for i_episode in count(1):\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        if render==\"human\":\n",
    "            env.render()\n",
    "        policy.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if render==\"rgb_array_list\":\n",
    "        save_video(env.render(),\n",
    "               \"videos\", fps=int(env.metadata[\"render_fps\"]), step_starting_index=step_starting_index,\n",
    "               episode_index=i_episode, episode_trigger=save_episode)\n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    finish_episode()\n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "            i_episode, ep_reward, running_reward))\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        torch.save(policy.state_dict(), 'cartpole_policy.pth')\n",
    "        break"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autonomous play..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils import play\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Allows for loading and running a pre-trained model,\n",
    "# or for the user to interact with the environment.\n",
    "# See 'cartpole_train' for more details.\n",
    "\n",
    "USER_PLAY = False\n",
    "MODEL_DIR = \"cartpole_policy.pth\"\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "if USER_PLAY:\n",
    "    env.reset()\n",
    "    key_action_map = {(ord('a'),): 0, (ord('d'),): 1}  # Map 'a' and 'd' to going left and right\n",
    "    play.play(env, keys_to_action=key_action_map, fps=15)  # Lower fps cause its too hard otherwise\n",
    "    sys.exit()\n",
    "\n",
    "# 0: Left, 1: Right\n",
    "action_size = env.action_space.n\n",
    "# 0: Cart position, 1: Cart Velocity, 2: Pole Angle, 3: Pole Angular Velocity\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "model = Policy()\n",
    "model.load_state_dict(torch.load(MODEL_DIR))\n",
    "model.eval()\n",
    "n_episodes = 40\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.asarray(state[0])\n",
    "\n",
    "    done = False\n",
    "    time_step = 0\n",
    "\n",
    "    score = 0\n",
    "    while not done and time_step < 500:\n",
    "        env.render()\n",
    "\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "        score += reward\n",
    "        reward = 0.1 if not done else -1\n",
    "        next_state = np.asarray(next_state)\n",
    "\n",
    "        state = next_state\n",
    "        time_step += 1\n",
    "\n",
    "    print(\"episode: {}, time steps: {}, score: {}\".format(e, time_step, score))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
