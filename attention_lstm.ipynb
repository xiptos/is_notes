{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiptos/is_notes/blob/main/attention_lstm.ipynb)",
   "id": "c19e6f098b81a1ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Attention Mechanism\n",
    "\n",
    "The Attention mechanism is inspired by how humans focus on different parts of an input when performing a task. In the context of neural networks, it allows the model to dynamically assign weights to different parts of the input sequence, highlighting the most relevant information.\n",
    "\n",
    "The general steps of the Attention mechanism are:\n",
    "\n",
    "- Calculate the alignment scores between the query (usually the current hidden state) and each element in the key sequence (usually the sequence of hidden states from an encoder).\n",
    "\n",
    "- Apply a softmax function to the alignment scores to get the attention weights.\n",
    "Compute the weighted sum of the values (usually the same as the keys) using the attention weights.\n",
    "\n",
    "Based on [https://www.codegenes.net/blog/attention-lstm-pytorch/](https://www.codegenes.net/blog/attention-lstm-pytorch/)"
   ],
   "id": "51493dce8dec60be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Attention LSTM\n",
    "\n",
    "An Attention LSTM combines the power of LSTM for sequential data processing and the Attention mechanism for focusing on relevant parts of the sequence. In an Attention LSTM, the Attention mechanism is typically applied after the LSTM layer to help the model better understand the importance of different time steps in the sequence."
   ],
   "id": "1f422513943b8e0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# The model",
   "id": "1fd44d54085bec35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) of token ids\n",
    "        \"\"\"\n",
    "        emb = self.embedding(x)             # (B, T, E)\n",
    "        lstm_out, _ = self.lstm(emb)        # (B, T, H)\n",
    "        scores = self.attn(lstm_out)        # (B, T, 1)\n",
    "        attn_weights = F.softmax(scores, dim=1)      # (B, T, 1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)  # (B, H)\n",
    "        out = self.fc(context)              # (B, output_dim)\n",
    "\n",
    "        if return_attention:\n",
    "            return out, attn_weights.squeeze(-1)     # (B, T)\n",
    "        return out"
   ],
   "id": "e6c94cc57932af3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sentences = [\n",
    "    \"i love cats\",\n",
    "    \"i like dogs\",\n",
    "    \"i hate homework\",\n",
    "    \"homework is bad\",\n",
    "    \"cats are great\",\n",
    "]\n",
    "\n",
    "labels = [1, 1, 0, 0, 1]  # 1=positive, 0=negative (toy)\n",
    "\n",
    "# --- build vocab ---\n",
    "PAD = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "tokens = [w for s in sentences for w in s.split()]\n",
    "freqs = Counter(tokens)\n",
    "vocab = {PAD: 0, UNK: 1}\n",
    "for w in freqs:\n",
    "    vocab[w] = len(vocab)\n",
    "\n",
    "pad_idx = vocab[PAD]\n",
    "\n",
    "def encode(sent, max_len=None):\n",
    "    ids = [vocab.get(w, vocab[UNK]) for w in sent.split()]\n",
    "    if max_len is not None:\n",
    "        ids = ids[:max_len]\n",
    "        if len(ids) < max_len:\n",
    "            ids += [pad_idx] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "max_len = max(len(s.split()) for s in sentences)\n",
    "X = torch.tensor([encode(s, max_len) for s in sentences])   # (N, T)\n",
    "y = torch.tensor(labels)                                   # (N,)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = TextDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ],
   "id": "7fe400ff7d625bee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training the model",
   "id": "a7d7e0493d0416e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 16\n",
    "hidden_dim = 32\n",
    "output_dim = 2  # 2 classes\n",
    "\n",
    "model = AttentionLSTM(vocab_size, embed_dim, hidden_dim, output_dim, pad_idx).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y in loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_x)                # (B, 2)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} - loss: {total_loss/len(dataset):.4f}\")"
   ],
   "id": "6e1504f3b873d7a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # pick one example, e.g. index 0\n",
    "    x_ex = X[0].unsqueeze(0).to(device)   # (1, T)\n",
    "    logits, attn = model(x_ex, return_attention=True)  # attn: (1, T)\n",
    "\n",
    "attn = attn[0].cpu().numpy()             # (T,)\n",
    "tokens = sentences[0].split()\n",
    "T = len(tokens)\n",
    "attn = attn[:T]                          # ignore pads\n",
    "\n",
    "attn_2d = attn[np.newaxis, :]           # (1, T)\n",
    "\n",
    "plt.figure(figsize=(4, 1.5))\n",
    "plt.imshow(attn_2d, cmap=\"viridis\", aspect=\"auto\")\n",
    "plt.colorbar(label=\"Attention weight\")\n",
    "plt.yticks([])\n",
    "plt.xticks(range(T), tokens, rotation=45, ha=\"right\")\n",
    "plt.title(\"Attention over words\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1f60c53ad42a5d30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "dbb6b1e46a17bbb8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
