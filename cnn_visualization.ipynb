{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Filters and Feature Maps in Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import cv2 as cv\n",
    "import argparse\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# load the model\n",
    "model = models.alexnet(pretrained=True)\n",
    "print(model)\n",
    "# get all the model children as list\n",
    "model_children = list(model.children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Convolutional Layer Filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Change `layer` value, to visualize each layer of the AlexNet. \n",
    "\n",
    "You should also define the y variable to visualize the convolutional filters. For this, you need to divide the shape of the first element of  `conv_layers`[`layer`].`weight` tensor, by 8 (you should also convert this to integer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter to keep count of the conv layers\n",
    "counter = 0 \n",
    "model_weights = []\n",
    "conv_layers = []\n",
    "\n",
    "for i in range(len(model_children)):\n",
    "    if type(model_children[i]) == nn.Conv2d:\n",
    "        counter += 1\n",
    "        model_weights.append(model_children[i].weight)\n",
    "        conv_layers.append(model_children[i])\n",
    "    elif type(model_children[i]) == nn.Sequential:\n",
    "        for child in model_children[i].children():\n",
    "            if type(child) == nn.Conv2d:\n",
    "                counter += 1\n",
    "                model_weights.append(child.weight)\n",
    "                conv_layers.append(child)\n",
    "print(f\"Total convolutional layers: {counter}\")\n",
    "\n",
    "# take a look at the conv layers and the respective weights\n",
    "for weight, conv in zip(model_weights, conv_layers):\n",
    "    print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 17))\n",
    "\n",
    "# Change layer value\n",
    "layer = ...\n",
    "for i, filter in enumerate(model_weights[layer]):\n",
    "    # Define y\n",
    "    y = ...\n",
    "    plt.subplot(8, y, i+1) \n",
    "    plt.imshow(filter[0, :, :].detach(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Reading an image and defining the transforms\n",
    "\n",
    "In this exercise, you must use any image of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and visualize an image\n",
    "img = cv.imread(...)\n",
    "img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "# define the transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img = np.array(img)\n",
    "# apply the transforms\n",
    "img = transform(img)\n",
    "print(img.size())\n",
    "# unsqueeze to add a batch dimension\n",
    "img = img.unsqueeze(0)\n",
    "print(img.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the Input Image Through Each Convolutional Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the image through all the layers\n",
    "results = [conv_layers[0](img)]\n",
    "for i in range(1, len(conv_layers)):\n",
    "    # pass the result from the last layer to the next layer\n",
    "    results.append(conv_layers[i](results[-1]))\n",
    "# make a copy of the `results`\n",
    "outputs = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the Feature Maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "layer_viz = outputs[layer][0, :, :, :]\n",
    "layer_viz = layer_viz.data\n",
    "print(layer_viz.size())\n",
    "for i, filter in enumerate(layer_viz):\n",
    "    plt.subplot(8, y, i + 1)\n",
    "    plt.imshow(filter, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly use pre-trained AlexNet for Image Classification and Visualization of the activation maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and data transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F \n",
    "import torchvision.utils as utils\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),       # resize the input to 224x224\n",
    "    transforms.ToTensor(),              # put the input to tensor format\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalize the input\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your image\n",
    "img = Image.open(...)\n",
    "\n",
    "print(\"original image's shape: \" + str(img.size))\n",
    "# pre-process the input\n",
    "transformed_img = data_transforms(img)\n",
    "print(\"transformed image's shape: \" + str(transformed_img.shape))\n",
    "# form a batch with only one image\n",
    "batch_img = torch.unsqueeze(transformed_img, 0)\n",
    "print(\"image batch's shape: \" + str(batch_img.shape))\n",
    "\n",
    "# load pre-trained AlexNet model\n",
    "print(\"\\nfeed the input into the pre-trained alexnet to get the output\")\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "# put the model to eval mode for testing\n",
    "alexnet.eval()\n",
    "\n",
    "# obtain the output of the model\n",
    "output = alexnet(batch_img)\n",
    "print(\"output vector's shape: \" + str(output.shape))\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Visualize activation maps.\n",
    "\n",
    "In this exercise you need to define the variables `l1`, `l2`, `l3`, `l4` and `l5` (convolutional layer 1, convolutional layer 2, convolutional layer 3, convolutional layer 4 and convolutional layer 5). Note that these values should correspond to the index of each convolutional layer of the AlexNet (HINT: Check the layers of the model, printed in the previous cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = utils.make_grid(batch_img, nrow=1, normalize=True, scale_each=True)\n",
    "img = I.permute((1, 2, 0)).cpu().numpy()\n",
    "\n",
    "l1, l2 ,l3 ,l4 ,l5 = ..., ..., ..., ..., ...\n",
    "\n",
    "conv_results = []\n",
    "x = batch_img\n",
    "for idx, operation in enumerate(alexnet.features):\n",
    "    x = operation(x)\n",
    "\n",
    "    if idx in {l1, l2, l3, l4, l5}:\n",
    "        conv_results.append(x)\n",
    "\n",
    "for i in range(5):\n",
    "    conv_result = conv_results[i]\n",
    "    N, C, H, W = conv_result.size()\n",
    "\n",
    "    mean_acti_map = torch.mean(conv_result, 1, True)\n",
    "    mean_acti_map = F.interpolate(mean_acti_map, size=[224,224], mode='bilinear', align_corners=False)\n",
    "\n",
    "    map_grid = utils.make_grid(mean_acti_map, nrow=1, normalize=True, scale_each=True)\n",
    "    map_grid = map_grid.permute((1, 2, 0)).mul(255).byte().cpu().numpy()\n",
    "    map_grid = cv2.applyColorMap(map_grid, cv2.COLORMAP_JET)\n",
    "    map_grid = cv2.cvtColor(map_grid, cv2.COLOR_BGR2RGB)\n",
    "    map_grid = np.float32(map_grid) / 255\n",
    "\n",
    "    visual_acti_map = 0.6 * img + 0.4 * map_grid\n",
    "    tensor_visual_acti_map = torch.from_numpy(visual_acti_map).permute(2, 0, 1)\n",
    "\n",
    "    plt.imshow(visual_acti_map)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1f132da1c524c70bbaa36195da4d87184587d2c19198205fdef91a8dde9a007"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
