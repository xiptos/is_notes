{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiptos/is_notes/blob/main/model_sel.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWlV6OxwMKhU"
   },
   "source": [
    "# Model Selection\n",
    "\n",
    "The task of producing the most suitable predictive model from a dataset has many nuances. In particular, there are two aspects that need to be defined experimentally before starting to adjust a model:\n",
    "\n",
    "* At first, it may not be clear which learning algorithm is best suited to be used in a given situation.\n",
    "\n",
    "* In addition, each learning algorithm has its own set of *hyperparameters*, which are the parameters of the algorithm itself, to counter the concept of *parameter* of the ML model to be adjusted.\n",
    "\n",
    "Model selection is the process of selecting one of different learning algorithms (e.g., SVM, decision tree learning, logistic regression, etc.) or, once the learning algorithm is selected, choosing between different values ​​of hyperparameters (e.g., in polynomial regression, deciding which polynomial degree to use during training).\n",
    "\n",
    "For a given prediction task, the choice of the most appropriate learning algorithm (and its respective hyperparameter values) is made based on some *estimate* of the generalization error obtained on unused data during training. In other words, different candidate models are evaluated during model selection. The model that has the best predictive performance (according to some pre-established assessment measure) is selected as a result of the model selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_HvrJRmzAqkh"
   },
   "source": [
    "# Two-way holdout method\n",
    "\n",
    "In classification or regression tasks, it is very common to split the available observations (original dataset ) in two datasets: \n",
    "\n",
    "* training dataset: used for training a model M\n",
    "* test dataset: used to estimate the *generalization error* of M\n",
    "\n",
    "This splitting method is commonly named *two-way holdout method*, or simply *holdout method*. This two-way split is illustrated in the image below ([source](https://www.datavedas.com/holdout-cross-validation/)).\n",
    "\n",
    "![alt text](https://www.datavedas.com/wp-content/uploads/2018/04/image001-768x368.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_Wx4vawBl9n"
   },
   "source": [
    "In Scikit-Learn, we can apply the holdout method by using the `train_test_split` function. This function receives as input a data matrix $X$ and response vector $y$.\n",
    "\n",
    "The following image illustrates the effect of applying the function `train_test_split`. As a result, two data matrices and two response vectors are created. \n",
    "\n",
    "![alt text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543836883/image_6_cfpjpr.png)\n",
    "\n",
    "Please notice that, each time `train_test_split` is called, it randomly selects which examples to put in the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vvv1GLjaNKI1"
   },
   "source": [
    "## Three-way holdout method\n",
    "\n",
    "The application of the two-way holdout method can lead to a misleading estimate of the generalization error if the test data is also used for model selection.\n",
    "\n",
    "Since the test dataset should ideally be used only to produce an estimate of the generalization error **after** training, it is common (at least for relatively large data sets) to further divide the training set into two subsets. The result of this is the production of three datasets, as described below.\n",
    "\n",
    "* Training set: used to adjust the parameters for several candidate models.\n",
    "\n",
    "* Validation set: used to adjust the hyperparameters of the ML algorithm. The predictive performance on this set is used for model selection. i.e., choose the best candidate model.  \n",
    "\n",
    "* Test set: used once to obtain an estimate of the generalization error for the chosen candidate model.\n",
    "\n",
    "The image below ([source](https://www.datavedas.com/holdout-cross-validation/)) illustrates the three-way split of the original dataset.\n",
    "\n",
    "![alt text](https://www.datavedas.com/wp-content/uploads/2018/04/image003.jpg)\n",
    "\n",
    "The image below ([source](https://datascience.stackexchange.com/questions/52632/cross-validation-vs-train-validate-test)) summarizes the model selection process for a given learning algorithm using the three data sets described above. Initially, the original data set is divided into two subsets, training and testing. Then, the training set is again divided into two, which generates the training, validation and test sets. After that, for each desired combination of hyperparameter values, a model is adjusted on the training set, and its performance on the validation set is computed. In this way, for each of the $ q $ combinations of hyperparameter values, $ q $ performance estimates are obtained. The predictive model corresponding to the combination of hyperparameter values ​​that produces the best performance estimate is selected. Finally, the generalization error estimate for this selected model is produced using the test set.\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/osBuF.png)\n",
    "\n",
    "The procedure illustrated in the figure above can be performed once for each of the different learning algorithms candidates in a supervised learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Vq5YxWvN5_F"
   },
   "source": [
    "### Split proportions\n",
    "\n",
    "Reserving  a test dataset to compute an estimate of the generalization error has the negative side effect of retaining potentially valuable information that the learning algorithm could benefit from. Therefore, choosing the size of the test set  involves a trade-off: allocating a very large test set can retain a lot of information that could be used to improve model fit during training. On the other hand, the smaller the test set, the more inaccurate the generalization error estimate is.\n",
    "\n",
    "In most practical cases, the most used divisions are 60:40,\n",
    "70:30 or 80:20, depending on the size of the initial data set.\n",
    "\n",
    "However, for large data sets, 90:10 or even 99: 1 may be appropriate, since 10% (or even 1%) of a very large sample can still be statistically significant.\n",
    "\n",
    "A common practice is to re-train the estimator on the entire data set (i.e., rejoining the training and test sets), in order to improve predictive performance of the resulting final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piO8DLsf93Cv"
   },
   "source": [
    "## $k$-fold Cross-validation\n",
    "\n",
    "There are two disadvantages of the holdout method:\n",
    "\n",
    "* the performance estimate can be very sensitive to the way used to subdivide the training set into the training and validation sets: this estimate will vary for different data samples. Repeating the holdout validation on different subsets of the data would provide a better estimate of the algorithm's performance.\n",
    "\n",
    "* a significant part of the data is not used for training the model. For example, if the split ratio is 50%, half of the data set does not contribute to model training! This can cause problems, especially if the initial data set is small.\n",
    "\n",
    "A more robust technique for performing model selection is *k-fold cross-validtion*. This technique corresponds to applying the holdout method $k$ times over $k$ different configurations of training and validation subsets.\n",
    "\n",
    "The following figure ([source](https://sebastianraschka.com/faq/docs/evaluate-a-model.html)) illustrates $k$-fold cross validation. This is an iterative technique. In the first iteration, the training set is subdivided into $k$ disjunct sets (in the figure, $k = 10$). Then, $k-1$ of these subsets are used to fit the predictive model. Once the model has been adjusted, its error is estimated in the examples of the subset that was not used during the adjustment, which produces the value $E_1$ in the figure. In the second iteration, another fold is selected to compute the estimate and again a model is adjusted, which allows to compute another estimate of the error, $E_2$. In all, $k$ iterations are performed, which results in $k$ performance estimates. Finally, a more robust (i.e., less biased) estimate of the model error is obtained by computing the average of the previously produced $k$ estimates.\n",
    "\n",
    "![alt text](https://sebastianraschka.com/images/faq/evaluate-a-model/k-fold.png)\n",
    "\n",
    "Keep in mind that, as k increases:\n",
    "* Bias in the estimation of the generalization error decreases;\n",
    "Computational cost also increases.\n",
    "* Empirical evidence shows that k=10 is a good value for datasets of moderate size.\n",
    "* For large sized datasets, k can be safely decreased.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LUXqo-lsHaPo"
   },
   "source": [
    "### Function cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJu2J83AX-NW"
   },
   "source": [
    "In the following example, the $k$-fold cross-validation technique is applied, with $k = 2$. The result is two performance estimates, which can be combined (for example, taking the average) to obtain a better measure of the model's generalization performance. This specific form of cross-validation is double cross-validation: we split the original data into two subsets and use each one as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "D33MZ2_iXSwY",
    "outputId": "d34e8d88-5451-44ba-a6bf-c760a4a331f1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Configures k-NN with 1 neighbor\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# divide the original dataset in 2 equal parts\n",
    "X1, X2, y1, y2 = train_test_split(X, y, random_state=0, train_size=0.5)\n",
    "\n",
    "# ajusta e avalia DOIS modelos\n",
    "\n",
    "y2_model = model.fit(X1, y1).predict(X2)\n",
    "\n",
    "y1_model = model.fit(X2, y2).predict(X1)\n",
    "\n",
    "accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jOlBbOMaGw1h"
   },
   "source": [
    "Implementing $k$-fold cross validation using the approach illustrated in the previous example can be time consuming for greater values of $k$.\n",
    "\n",
    "Scikit-Learn provides the function [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html). This function makes it easier to apply $k$-fold cross validation. \n",
    "\n",
    "It returns a NumPy array with an accuracy value for each fold. Notice that, for each fold, the training and tests sets will be different. Therefore, different accuracy values for each fold will be produced. Also notice that we can compute the mean and standard deviation of the resulting accuracies.\n",
    "\n",
    "The following example illustrates the application of the function cross_val_score to the case $k = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "tSLzxCB_YERc",
    "outputId": "70c675e7-13a7-4f4b-89c6-fb35ef94bbf5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=10)\n",
    "print('Scores in each fold: ', scores)\n",
    "print('Average score: ', scores.mean())\n",
    "print('Std deviation of scores: ', scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUefpkmoY4v2"
   },
   "source": [
    "Scikit-Learn implements several cross-validation schemes that are useful in particular situations; these are implemented through iterators in the cross_validation module. In practice, we have to define the iterator through the `cv` parameter.\n",
    "\n",
    "As an example, we might want to go to the extreme case where the number of subsets is equal to the number of examples contained in the training set. In this case, in each iteration, a model is fitted using all but one example, and accuracy is measured against the only example not used during training. This type of cross-validation is known as **leave-one-out cross-validation**.\n",
    "\n",
    "The example below illustrates the use of this particular case leave-one-out cross-validation. Since there are 150 samples, the leave-one-out cross-validation produces 150 estimates (1.0 indicates a successful forecast, and 0.0 indicates an unsuccessful one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "paZ5oipGZIoJ",
    "outputId": "dbf573c2-143c-443d-8011-52720752e43f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=LeaveOneOut())\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APSamO7pZZ2F"
   },
   "source": [
    "The average of these 150 values ​​corresponds to the estimated error rate of the model produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "aXh4DFwqZgeG",
    "outputId": "24c45ba7-c67b-4f11-9488-85b72d42bfb1"
   },
   "outputs": [],
   "source": [
    "print('Average score: ', scores.mean())\n",
    "print('Std deviation of scores: ', scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yHk15WRYiziA"
   },
   "source": [
    "## Stratified k-fold cross-validation\n",
    "\n",
    "In the context of the classification task, an unbalanced presents as unequal proportion of examples for each class. See the image below ([source](https://www.researchgate.net/publication/306376881_Survey_of_resampling_techniques_for_improving_classification_performance_in_unbalanced_datasets)) for an example.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/698/1*cd6AorHoJYMFyj7IZd2nOg.png)\n",
    "\n",
    "For unbalanced datasets, if the vanilla version of $k$-fold cross-validation is used, it can be the case that one or more produced folds end up with zero examples of a particular class. In such situations, it is better to use *stratified $k$-fold cross-validation*. In stratified $k$ cross-validation, class proportions are preserved in each of the $k$ subsets, so that each one is representative of the class proportions in the training data set. The image below (source) illustrates how stratified $k$-fold crors-validation works.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/562/0*QKJTHrcriSx2ZNYr.png)\n",
    "\n",
    "In Scikit-Learn, the `StratifiedKFold` class implements the stratified cross-validation method. See the following example, that applies stratified cross-validation to a toy dataset with two classes. Notice that the proportions os classes in each fold is approximately the same as the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "bB_y_3iml9Gs",
    "outputId": "05f0b464-abde-4d13-b703-f4358b62ba22"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# data matrix\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [5, 6], [7, 8], \n",
    "              [9, 10], [9, 10], [11, 12], [11, 12],\n",
    "              [1, 2], [3, 4], [5, 6], [7, 8], [4, 5], [5, 6], [7, 8], [4, 5]])\n",
    "\n",
    "# response vector\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "print(skf)  \n",
    "\n",
    "# print the indices of examples in each fold\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "   X_train, X_test = X[train_index], X[test_index]\n",
    "   y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJw6vFxDix6O"
   },
   "source": [
    "# Hyperparameter optimization\n",
    "\n",
    "In contrast to the model parameters that are learned during training, the values ​​of the hyperparameters are set before the training of a model begins. Some examples of hyperparameters are given below.\n",
    "\n",
    "* the learning rate in logistic regression and in linear regression, \n",
    "* the height of trees in decision tree learning,\n",
    "* the value of $k$ in $k$-NN, \n",
    "* the value of $k$ in $k$-means, \n",
    "* the values of $\\epsilon$ and $\\operatorname{minPoints}$ in DBSCAN,\n",
    "* the regularization term in linear regression,\n",
    "* the polynomial degree in polynomial regression.\n",
    "\n",
    "\n",
    "Hyperparameters can be considered as configurations of a learning algorithm. In general, the ideal settings for the algorithm to generate a suitable model for one dataset may not be the same for another dataset. \n",
    "\n",
    "*Hyperparameter optimization* (aka *hyperparameter tuning*) is the procedure of defining appropriate values for the hyperparameters of a given learning algorithm. The general approach is to generate several combinations of hyperparameter values. Then, a model is trained for each combinatation. Finally, the best combination (according to some evaluation measure applied on the candidate models) is chosen.\n",
    "\n",
    "When the learning algorithm generates an estimator (either a classifier or a regressor), there are two main strategies to *automatically* tune its hyperparameters: *Grid Search* and *Random Search*. Both strategies generate several hyperparameter combinations. Then, for each combination, an estimator is trained a model (on the training set) and evaluateed (on the validation set). The difference between these two strategies is in the way the several combinations are generated, as described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iB4Yw9xzVy4j"
   },
   "source": [
    "**Grid search** first defines a grid of hyperparameter combinations. The amount of hyperparameters defines the number of dimensions in the grid. Then, each combination (a point in the grid) ​​is used to produce a model.\n",
    "\n",
    "The image below ([source](https://www.researchgate.net/publication/271771573_TuPAQ_An_Efficient_Planner_for_Large-scale_Predictive_Analytic_Queries/figures?lo=1)) illustrates the GS procedure for a learning algorithm with two hyperparameters. The star represents the combination of hyperparameters that gives the best model. as measured in the validation set. The red dot in picture on the left represents the first combination test by GS. The picture on the center represents the second combination. The picture on the right illustrates all the tested combinations in the 2-dimensional grid.\n",
    "\n",
    "![texto alternativo](https://www.researchgate.net/profile/Michael_Jordan13/publication/271771573/figure/fig4/AS:668513593217027@1536397469229/Illustration-of-naive-grid-search-for-a-single-model-family-with-two-hyperparameters_W640.jpg)\n",
    "\n",
    "**Random search** selects random combinations of hyperparameter values ​​(within preconfigured ranges of values for each hyperparameter) to train and evaluate the candidate models. Instead of trying all possible combinations of values, RS performs a pre-defined number of iterations, testing a random combination of hyperparameters ​​in each iteration.\n",
    "\n",
    "Both GS and RS can be very computationally expensive. For example, searching for 20 different values ​​for each of 4 hyperparameters would require 160,000 k-fold cross-validation runs. If $k = 10$ then 1,600,000 model adjustments and 1,600,000 evaluations should be performed\n",
    "\n",
    "The image below ([source](https://community.alteryx.com/t5/Data-Science/Hyperparameter-Tuning-Black-Magic/ba-p/449289)) illustrates the difference between GS and RS. Think of a learning algorithm with just two hyperparameters. This way, each combination of its hyperparameters is a pair of numbers. Suppose one of these hyperparameters (x-axis) has more influence than the other (y-axis) on the predictive performance of the generated models. The plot on the left shows several of these pairs organized in a grid. In total, there are thirty combination in this grid. The picture on the right shows other combinations of pairs; this times these combinations where randomly selected. Notice that RS has the potential to explore more promising combinations than GS.\n",
    "\n",
    "![texto alternativo](https://pvsmt99345.i.lithium.com/t5/image/serverpage/image-id/74545i97245FDAA10376E9/image-size/large?v=1.0&px=999)\n",
    "\n",
    "In Scikit-Learn, the classes [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) implement GS and RS, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQg_3AcwaZjQ"
   },
   "source": [
    "### Class GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n59pZ0EI4I_d"
   },
   "source": [
    "Scikit-Learn provides the class `GridSearchCV` to perform grid search.\n",
    "\n",
    "The following code block presents an example of using the `GridSearchCV` class to find the ideal polynomial model for a data set. In this example, a three-dimensional grid of hyperparameters is explored:\n",
    "\n",
    "* the polynomial degree,\n",
    "* a boolean indicator that indicates whether the linear coefficient should be adjusted,\n",
    "* an indicator (boolean) indicating whether the data should be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HyyBkg2E0eAP"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def polinomial_regression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAW_e44vz86E"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': np.arange(21),\n",
    "              'linearregression__fit_intercept': [True, False]}\n",
    "\n",
    "def make_data(N, err=1.0, rseed=1):\n",
    "    # randomly sample the data\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1) ** 2\n",
    "    y = 10 - 1. / (X.ravel() + 0.1)\n",
    "    if err > 0:\n",
    "        y += err * rng.randn(N)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "VYV99pMK0FB8",
    "outputId": "e95b5e59-aabd-45a8-88e7-243f803dcf45"
   },
   "outputs": [],
   "source": [
    "X, y = make_data(40)\n",
    "grid = GridSearchCV(polinomial_regression(), param_grid, cv=7)\n",
    "grid.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "colab_type": "code",
    "id": "Uw8ZQ4025wje",
    "outputId": "e282cb97-3bff-4748-a6a4-c9f52be86290"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()  # plot formatting\n",
    "\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "plt.scatter(X.ravel(), y, color='black')\n",
    "axis = plt.axis()\n",
    "for degree in [1, 3, 5]:\n",
    "    y_test = polinomial_regression(degree).fit(X, y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\n",
    "plt.xlim(-0.1, 1.0)\n",
    "plt.ylim(-2, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "vGm6ipNT0Frz",
    "outputId": "cb2cf362-75bc-46d4-c4f8-9f6596775724"
   },
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "colab_type": "code",
    "id": "7FXeoFiP5RTB",
    "outputId": "e7d2c3d6-4340-44da-cf65-9ed449949cc1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = grid.best_estimator_\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_test = model.fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test);\n",
    "plt.axis(lim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTlVoHTlY7EQ"
   },
   "source": [
    "### Class RandomizedSearchCV\n",
    "\n",
    "This allows you to explicitly control the number of parameter combinations that are attempted. The number of search iterations is defined based on time or resources. Scikit Learn offers the RandomizedSearchCV function for this process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "sNmXxgGsPhkZ",
    "outputId": "3c8445cd-0336-4491-db70-f43be404a3f2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def linear_SVC(x, y, param, kfold):\n",
    "    param_grid = {'C':param}\n",
    "    k = StratifiedKFold(n_splits=kfold)\n",
    "    grid = GridSearchCV(LinearSVC(dual=False), param_grid=param_grid, cv=k, n_jobs=4, verbose=1)\n",
    "\n",
    "    return grid.fit(x, y)\n",
    "\n",
    "def Linear_SVC_Rand(x, y, param, kfold, n):\n",
    "    param_grid = {'C':param}\n",
    "    k = StratifiedKFold(n_splits=kfold)\n",
    "    randsearch = RandomizedSearchCV(LinearSVC(dual=False), param_distributions=param_grid, cv=k, n_jobs=4,\n",
    "                                    verbose=1, n_iter=n)\n",
    "\n",
    "    return randsearch.fit(x, y)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "\n",
    "start = timer()\n",
    "param = [i/1000 for i in range(1,1000)]\n",
    "param1 = [i for i in range(1,101)]\n",
    "param.extend(param1)\n",
    "\n",
    "#progress = progressbar.bar.ProgressBar()\n",
    "clf = Linear_SVC_Rand(x=x_train, y=y_train, param=param, kfold=3, n=100)\n",
    "\n",
    "print('LinearSVC:')\n",
    "print('Best cv accuracy: {}' .format(clf.best_score_))\n",
    "print('Test set score:   {}' .format(clf.score(x_test, y_test)))\n",
    "print('Best parameters:  {}' .format(clf.best_params_))\n",
    "print()\n",
    "\n",
    "duration = timer() - start\n",
    "print('time to run RandomizedSearchCV: {}' .format(duration))\n",
    "\n",
    "\n",
    "print('########')\n",
    "\n",
    "#high C means more chance of overfitting\n",
    "\n",
    "start = timer()\n",
    "param = [i/1000 for i in range(1,1000)]\n",
    "param1 = [i for i in range(1,101)]\n",
    "param.extend(param1)\n",
    "\n",
    "clf = linear_SVC(x=x_train, y=y_train, param=param, kfold=3)\n",
    "\n",
    "print('LinearSVC:')\n",
    "print('Best cv accuracy: {}' .format(clf.best_score_))\n",
    "print('Test set score:   {}' .format(clf.score(x_test, y_test)))\n",
    "print('Best parameters:  {}' .format(clf.best_params_))\n",
    "print()\n",
    "\n",
    "duration = timer() - start\n",
    "print('time to run GridSearchCV: {}' .format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "bPaJ9EBODzzR",
    "outputId": "15c02ec0-e907-4a42-c66e-ea4e251f2ff6"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# get some data\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=5)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkB2DK0laaGr"
   },
   "source": [
    "## Model selection: practical tips\n",
    "\n",
    "* It is appropriate to start with a simple algorithm that can be used quickly.\n",
    "\n",
    "* Use learning curves to decide for more (or less) data, more (or less) attributes, etc.\n",
    "\n",
    "* Another technique used in model selection is *error analysis*, which corresponds to manually checking the examples (in the validation set) that the algorithm has erred. It is sometimes possible to detect a pattern of systematic error in these examples.\n",
    "\n",
    "* The motivation for using k-fold cross-validation is to be able to use a good amount of test or validation data, without significantly reducing the training data set. In situations where there is enough data to have a good-sized training data set, in addition to reasonable amounts of test and validation data, holdout cross validation can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GeybIdRaZvd"
   },
   "source": [
    "\n",
    "# Case Study: Housing dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fFMl9Q2dwfpw"
   },
   "source": [
    "In this case study, we will use the *Housing* dataset to practice some concepts related to hyperparameter optimization. This dataset contains information about residences in Boston/USA. There are 506 samples and 13 variables in this dataset. This dataset is used to train regressors, in which the objective is to predict the value of home prices (MEDV feature) using the predictive features provided. This dataset can be loaded with the Scikit-Learn library, according to the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zu6suAhQmRjp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import pandas as pd  \n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "cal_dataset = fetch_california_housing()\n",
    "\n",
    "print(cal_dataset.keys())\n",
    "print(cal_dataset['DESCR'])\n",
    "\n",
    "cal = pd.DataFrame(cal_dataset.data, columns=cal_dataset.feature_names)\n",
    "cal['MEDV'] = cal_dataset.target\n",
    "cal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSioPypYksrd"
   },
   "source": [
    "**(1)** \n",
    "\n",
    "a) Perform the split of the Housing dataset into training and test subsets using holdout strategy. Set aside 80% of the data for training. After that, build a OLS regression model with that split, and evaluate the MSE in the test dataset.\n",
    "\n",
    "b) Repeat the previous experiment, this time using $k$-fold cross validation instead of the hold-out strategy. Use $k = 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZpAEr8ntiEa"
   },
   "source": [
    "**(2)** Consider the following code block, which is intended to perform Grid Search on the `Housing` dataset. The `param_grid` parameter tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of` n_estimators` and `max_features` with hyperparameters specified in the first dict (don't worry about what these hyperparameters mean for the moment), and then try all 2 × 3 = 6 combinations of hyperparameter values ​​in the second dict, but this time with the `bootstrap` hyperparameter set to` False` instead of `True` (which is the default value).\n",
    "In short, GS will explore 12 + 6 = 18 combinations of hyperparameter values ​​for the `RandomForestRegressor` algorithm.\n",
    "\n",
    "  a) Based on the value of $k$ defined to perform the cross-validation in the code below, how many models will be adjusted during GS?\n",
    "\n",
    "  b) Complete the code given below to tune the hyperparameters using Grid Search. What is the RMSE value for the best model?\n",
    "  \n",
    "  c) Use Random Search with 18 iterations. Is there a change in the  produced best combination of hyperparameter values, when compared to Grid Search? What is the new RMSE value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "id": "4P0gkhvutlq2",
    "outputId": "9a834746-fb4b-433d-b5e3-cd1bfdefc423"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uW2yvX0wEbtD"
   },
   "source": [
    "To load the data, the `load_boston` function is imported. To standardize the data, the normalization function was used, which places the data with $ \\ mu = 0 $ and $ \\ sigma ^ 2 = 1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "upsQ2LjPFDFC",
    "outputId": "2e74fe20-a328-4fa0-f2e2-4459d00f1ec4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "cal_dataset = fetch_california_housing()\n",
    "\n",
    "cal = pd.DataFrame(normalize(cal_dataset.data), columns=cal_dataset.feature_names)\n",
    "cal['MEDV'] = cal_dataset.target\n",
    "cal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSNFYGeIFKvK"
   },
   "source": [
    "### Answer: 1a\n",
    "\n",
    "We can use the function `train_test_split` to create a training dataset with  80% of the examples, as requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "8XeiO1bdFMSp",
    "outputId": "18349b3b-3323-4e0e-8da0-329fe14044f2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = cal.drop(columns=['MEDV'])\n",
    "y = cal['MEDV']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    train_size=0.8, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print('MSE:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AjvFQrB8GLjw"
   },
   "source": [
    "### Answer: 1b\n",
    "\n",
    "For the k-fold validation, five data divisions were performed between training and testing using the cross_value function with $ MSE $ validation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "_DKu59nNGbVL",
    "outputId": "2aaa0c31-c6e7-4714-ec59-5de5f1eb2574"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = cal.drop(columns=['MEDV'])\n",
    "y = cal['MEDV']\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "scores = cross_validate(reg, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(scores)\n",
    "print(-scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "138_5SHdG3u0"
   },
   "source": [
    "### Answer: 2a\n",
    "\n",
    "In total, 90 models will be trained by Grid Search as there are 18 parameters that will be combined and trained 5 times each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DlOiV0NCXOk"
   },
   "source": [
    "### Answer: 2b\n",
    "The code below performs the training using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "i83XrXyhG8AT",
    "outputId": "c7b934c4-4181-45ef-d763-03e036dc0ded"
   },
   "outputs": [],
   "source": [
    "X,y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "housing = pd.concat([X,y], axis=1)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qlZ8no0G-7xH"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "housing_prepared = scaler.fit_transform(housing.drop(columns='MedHouseVal'))\n",
    "housing_labels = housing.loc[:,'MedHouseVal'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AYU3feCs_w9K",
    "outputId": "11d41e11-cb3c-462b-a5c3-ad18af5cedef"
   },
   "outputs": [],
   "source": [
    "housing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "d5OXx67cG-Nu",
    "outputId": "a15afce8-c95e-48aa-a61f-594b5f5cf1ff"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "%time grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rr4PgbUQHB1j",
    "outputId": "a1d69334-7d6b-454a-f328-e0337854061c"
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9G9pmH7OHFGK",
    "outputId": "5a88b92d-5111-403c-8d64-2b1fc2241e80"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(-grid_search.best_score_)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4H47tctDEa2"
   },
   "source": [
    "### Answer: 2c\n",
    "\n",
    "The code below performs the training using Random Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tcMgkMY1HImY",
    "outputId": "6bd6c7cd-47fd-45b3-b3a6-0183633cd43e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "random_search = RandomizedSearchCV(forest_reg, \n",
    "                                   param_grid, \n",
    "                                   cv = 5,\n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   n_iter = 18,\n",
    "                                   verbose = 2)\n",
    "%time random_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "crebtgnXHLNs",
    "outputId": "19210af6-aca8-4a64-b435-30c2d5b7986f"
   },
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "mJQhCxMHHNbg",
    "outputId": "f4804002-7884-496f-d229-2681e2221673"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rmse = np.sqrt(-random_search.best_score_)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cJuVwtxRjlkh"
   },
   "source": [
    "# Case Study: Wine Quality dataset\n",
    "\n",
    "Let us see another case study of applying hyperparameter optimization using the Wine Quality dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BNaX00jXwUt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read in and create target\n",
    "wine_data = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',sep=';')\n",
    "wine_data['high_quality'] = [1 if x >=7 else 0 for x in wine_data['quality']]\n",
    "\n",
    "# Split data\n",
    "X = wine_data.drop(['quality'], axis=1)\n",
    "y = wine_data['high_quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "79d2My1npcRP",
    "outputId": "0a2ca968-c4e8-4d45-f624-121d9772dd11"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read in and create target\n",
    "wine_df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',sep=';')\n",
    "\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFCCA0-B8zGK"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "08hFy2Ow80AU",
    "outputId": "0c1ca340-36ab-48d4-83c6-f2e999ce514e"
   },
   "outputs": [],
   "source": [
    "wine_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "colab_type": "code",
    "id": "gOUJtffE_4oW",
    "outputId": "d35f5c13-a733-4df5-f0f2-33a0d72961d7"
   },
   "outputs": [],
   "source": [
    "wine_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qaydnuG8oSi"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = wine_df.drop(['quality'], axis=1)\n",
    "y = wine_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKkyePbF_9Qm"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "AMIHfSgDldTX",
    "outputId": "558f7f1b-dff7-45f4-fbc0-047aebb4222c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {'C': [.01, .1, 1, 10, 100, 1000, 10000],\n",
    "                  'gamma': [0.0001, 0.001, 0.01, .1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Create grid search \n",
    "grid_search = GridSearchCV(SVC(),\n",
    "                           param_grid, \n",
    "                           cv=5)\n",
    "# Fit grid search\n",
    "%time grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "colab_type": "code",
    "id": "IF8SpsThm6fu",
    "outputId": "d097bf01-b240-4f5e-939f-a1f4e235c74b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pull results from grid search\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "hm_data = results.pivot(index='param_C', \n",
    "                   columns='param_gamma',\n",
    "                   values='mean_test_score')\n",
    "\n",
    "# Plot heatmap\n",
    "fix, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "ax = sns.heatmap(hm_data,\n",
    "                 annot=True,\n",
    "                 cmap='RdBu',\n",
    "                 linecolor='white')\n",
    "\n",
    "ax.set_title('Hyperparameter Gridsearch', fontsize=16)\n",
    "ax.set_ylabel('C Parameter', fontsize=12)\n",
    "ax.set_xlabel('Gamma Parameter', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "zVoAy7LjnTy-",
    "outputId": "c9ce8c34-e445-401a-af99-4175e01d01c3"
   },
   "outputs": [],
   "source": [
    "# Print best parameters and best score\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "### Best parameters: {'C': 100, 'gamma': 0.0001}\n",
    "### Best cross-validation score: 1.00\n",
    "\n",
    "# Plug best parameters into the model, train it, and test on held out data\n",
    "best_svc = SVC(**grid_search.best_params_).fit(X_train, y_train)\n",
    "best_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2FtRgBNnjKA"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "AjS1R5SLn-cn",
    "outputId": "28affd8a-69b0-4fc0-b0e8-58d9e3280e6d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = best_svc.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "Mmm8WrH6nnzW",
    "outputId": "c16dbffe-2981-4eaa-a507-142c37edc228"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "cDXK5Ti9rQWr",
    "outputId": "3d45ea0c-0fbf-43d6-cd0c-e516bc574dee"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {'C': [100, 500, 1000, 5000, 10000],\n",
    "                  'gamma': [0.001/3, 0.001/2, 0.001, 0.001*2, 0.001*3]}\n",
    "\n",
    "# Create grid search \n",
    "grid_search = GridSearchCV(SVC(),\n",
    "                           param_grid, \n",
    "                           cv=5)\n",
    "# Fit grid search\n",
    "%time grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "jHWLvg-Ir5s8",
    "outputId": "8b4ec278-5bf5-4d61-9543-b9455fe7af6b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pull results from grid search\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "hm_data = results.pivot(index='param_C', \n",
    "                   columns='param_gamma',\n",
    "                   values='mean_test_score')\n",
    "\n",
    "# Plot heatmap\n",
    "fix, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "ax = sns.heatmap(hm_data,\n",
    "                 annot=True,\n",
    "                 cmap='RdBu',\n",
    "                 linecolor='white')\n",
    "\n",
    "ax.set_title('Hyperparameter Gridsearch', fontsize=16)\n",
    "ax.set_ylabel('C Parameter', fontsize=12)\n",
    "ax.set_xlabel('Gamma Parameter', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Beta1kA0saS9",
    "outputId": "5227046b-1fcb-4a96-faa8-459c9ffe0e4c"
   },
   "outputs": [],
   "source": [
    "# Print best parameters and best score\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "### Best parameters: {'C': 100, 'gamma': 0.0001}\n",
    "### Best cross-validation score: 1.00\n",
    "\n",
    "# Plug best parameters into the model, train it, and test on held out data\n",
    "best_svc = SVC(**grid_search.best_params_).fit(X_train, y_train)\n",
    "best_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "moYVH8CDsz6Z",
    "outputId": "1c1d6738-ae0e-4dae-cc2c-b583847df088"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = best_svc.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "iuhCPa3BvF4i",
    "outputId": "a388c826-52f5-42a5-b120-b997707cee6f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOGaJFqwJ3eukQo+NsAYsNp",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ppcic-ml-modelselection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
