{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a75e14",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Linear regression is used for estimation or prediction. However, despite the name, logistic regression is used for classification, a problem of completely different nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71824758",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "Binary classification implies that the target variable can assume two values:\n",
    "\n",
    "$$y \\in [0,1] \\rightarrow \\left\\{ \\begin{array}{l}\n",
    "                                      \\text{0: negative class} \\\\\n",
    "                                      \\text{1: positive class}\n",
    "        \\end{array} \\right.$$\n",
    "        \n",
    "Logistic regression allows us to generate a probability of an object (the feature vector) to belong to a given class.\n",
    "\n",
    "Since we are looking for a probability, we would like to have a representation such that $0 \\leq h_\\theta(x) \\leq 1$.\n",
    "\n",
    "For that, we can use the sigmoid function:\n",
    "\n",
    "$$g(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Represented by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e63654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "\n",
    "def plot_sigmoid():\n",
    "  plt.grid()\n",
    "  plt.xlim([-10.0, 10.0])\n",
    "  plt.ylim([-0.1, 1.1])\n",
    "  xs = np.arange(-10, 10, 0.001)\n",
    "  plt.xlabel('$z$', size=10)\n",
    "  plt.ylabel('$g(z)$', size=10)\n",
    "  plt.title('Sigmoid function', size=10)\n",
    "  plt.plot(xs, sigmoid(xs), label=r'$g(z)= \\frac{1}{1+e^{-z}}$')\n",
    "  plt.legend(loc='lower right', fontsize=15)\n",
    "\n",
    "plot_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66020185",
   "metadata": {},
   "source": [
    "So, we can embedd the linear regression hypothesis ($\\theta^Tx$) in the sigmoid function:\n",
    "\n",
    "$$h_\\theta(x) = g(\\theta^T x) = \\frac{1}{1+e^{-\\theta^Tx}}$$\n",
    "\n",
    "thus estimating the probability of $y=1$ for a given $x$ considering $\\theta$:\n",
    "\n",
    "$$h_\\theta(x) = P(y=1|x;\\theta)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a0638",
   "metadata": {},
   "source": [
    "Notice that\n",
    "\n",
    "- the range of values that $g(z)$ produces is the open range $(0,1)$\n",
    "- $g(0)$ is equal to $0.5$\n",
    "- for ever-increasing $z$ values (in magnitude), $g(z)$ returns values ever closer to $0$ or $1$\n",
    "- $g(z) \\geq 0.5$ when $z \\geq 0$\n",
    "- $g(z) < 0.5$ when $z < 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19408ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('g(0.0) = %.3f' % sigmoid(0.0))\n",
    "print('g(0.5) = %.3f' % sigmoid(0.5))\n",
    "print('g(-0.5) = %.3f' % sigmoid(-0.5))\n",
    "print('g(10.0) = %.3f' %  sigmoid(10.0))\n",
    "print('g(-10.0) = %.3f' % sigmoid(-10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de6211",
   "metadata": {},
   "source": [
    "To give you a more concrete view of what a logistic regression model does, consider a binary classification problem over a training set with 2 predictors, $x_1$ and $x_2$. Also, consider that the logistic regression algorithm produced a model represented by the vector $\\theta = [-3, 1, 1]$. The code cell below is a toy implementation of a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dot_product(theta, x):\n",
    "    # Compute the weighted sum.\n",
    "    return np.dot(x, theta)\n",
    "\n",
    "def predict(theta, x):\n",
    "    y_prob = proba(theta,x)\n",
    "    return np.round(y_prob).astype(int)\n",
    "\n",
    "def proba(theta, x):\n",
    "    # Returns the probability that x belongs to the positive class\n",
    "    return sigmoid(dot_product(theta, x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb99fd2",
   "metadata": {},
   "source": [
    "Further, consider that we want to determine (predict) the class for the examples $[3,3]$ and $[0.4,1.3]$. The following code produces the probability value that $\\mathbf{x}$ belongs to the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582909f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.asarray([-3, -1, -1]).T\n",
    "\n",
    "X = np.asarray([[3, 3],[0.4, 1.3]])\n",
    "\n",
    "print('Example(s):\\n', X)\n",
    "\n",
    "col = np.ones((2,1))\n",
    "X = np.append(col, X, axis=1)\n",
    "\n",
    "print('Predicted class(es) for each example:', predict(theta, X))\n",
    "print('Probability of each example belonging to the positive class:', proba(theta, X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4409aa",
   "metadata": {},
   "source": [
    "In the example above:\n",
    "\n",
    "1. How do you interpret the output when $\\theta = [-3, 1, 1]$? \n",
    "2. What happens if you change $\\theta$ to $[-3, -1, -1]$?\n",
    "3. What happens if you keep $\\theta = [-3, 1, 1]$ and try to classify the example $\\mathbf{x}=[0.4,1.3]$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67851062",
   "metadata": {},
   "source": [
    "## Decision boundaries\n",
    "\n",
    "We can have a graphical intuition of the example above. Again, suppose $\\theta = [-3, 1, 1]T$. Then to classify an example $\\mathbf{x} = (x_1, x_2)$:\n",
    "\n",
    "$$\n",
    "\\theta^T \\cdot \\mathbf{x} = [-3, 1, 1]^T \\cdot [1,x_1,x_2] = -3 + x_1 + x_2\n",
    "$$\n",
    "\n",
    "Recall that, if $(\\theta^T \\cdot \\mathbf{x}) \\geq 0$, then $\\mathbf{x}$ is classified as positive. That is:\n",
    "\n",
    "$$\n",
    "-3 + x_1 + x_2 \\geq 0 \\rightarrow x_1 + x_2 \\geq 3\n",
    "$$\n",
    "\n",
    "The equation $x_1 + x_2 = 3$ actually defines what we call a *decision boundary*. According to the Wikipedia, the definition of a decision boundary is the following:\n",
    "\n",
    "> In a classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.\n",
    "\n",
    "In the case of a 2-dimensional dataset, as the space of predictors is bidimentional, the decision boundary is a simple line in the cartesian plane. This line splits the space of features in two subregions, positive and negative. See the plot below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b18532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "point1 = [0, 3]\n",
    "point2 = [3, 0]\n",
    "\n",
    "x_values = [point1[0], point2[0]]\n",
    "y_values = [point1[1], point2[1]]\n",
    "\n",
    "plt.plot(x_values, y_values, c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d961b",
   "metadata": {},
   "source": [
    "In particular, all the examples that fall above this line are classified as being positive, while the examples predicted as negative are located below this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d85f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = np.array([\n",
    "    [0.4, 1.3],\n",
    "    [3, 3]\n",
    "])\n",
    "\n",
    "col = np.ones((2,1))\n",
    "x = np.append(col, data, axis=1)\n",
    "\n",
    "y_pred = predict(theta, x)\n",
    "print(y_pred)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.scatter(data[:,0], data[:, 1], c=np.array([0,1]), s=50,\n",
    "           cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "           edgecolor=\"white\", linewidth=1)\n",
    "\n",
    "plt.plot(x_values, y_values, c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99e11c",
   "metadata": {},
   "source": [
    "Let us draw the decision boundary defined by the vector $\\theta = [-3, 1, 1]$ along with the examples in the toy dataset presented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[-0.1, 1.4],\n",
    "              [-0.5,-0.1],\n",
    "              [ 1.3, 0.9],\n",
    "              [-0.6, 0.4],\n",
    "              [-1.5, 0.4],\n",
    "              [ 0.2, 0.2],\n",
    "              [-0.3,-0.4],\n",
    "              [ 0.7,-0.8],\n",
    "              [ 1.1,-1.5],\n",
    "              [-1.0, 0.9],\n",
    "              [-0.5,-1.5],\n",
    "              [-1.3,-0.4],\n",
    "              [-1.4,-1.2],\n",
    "              [-0.9,-1.1],\n",
    "              [ 0.4,-1.3],\n",
    "              [-0.4, 0.6],\n",
    "              [ 0.3,-0.5]])\n",
    "\n",
    "y = np.array([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1])\n",
    "\n",
    "plt.style.use('classic')\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "colormap = np.array(['r', 'b'])\n",
    "\n",
    "def plot_scatter(X, y, colormap):\n",
    "  plt.grid()\n",
    "  plt.xlim([-5.0, 5.0])\n",
    "  plt.ylim([-5.0, 5.0])\n",
    "  plt.xlabel('$x_1$', size=20)\n",
    "  plt.ylabel('$x_2$', size=20)\n",
    "  plt.title('Input 2D points', size=18)\n",
    "  plt.scatter(X[:,0], X[:, 1], s=50, c=colormap[y])\n",
    "\n",
    "plot_scatter(X, y, colormap)\n",
    "\n",
    "point1 = [-5, 8]\n",
    "point2 = [5, -2]\n",
    "\n",
    "x_values = [point1[0], point2[0]]\n",
    "y_values = [point1[1], point2[1]]\n",
    "\n",
    "plt.plot(x_values, y_values, c='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c966e0",
   "metadata": {},
   "source": [
    "By analysing the image above, it should be clear that the decision boundary defined by the vector $\\theta = [-3, 1, 1]$ is **not** the best one, in the sense that it does not correctly classify many of the examples in the training dataset. In particular, the *prediction error* of the corresponding classifier is $8/17 \\approx 47\\%$, which is worst than tossing a coin to determine the class of an new example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c9cbeb",
   "metadata": {},
   "source": [
    "## Optimization problems\n",
    "\n",
    "By now, you should understand that the goal in logistic regression is to find the components of the vector $\\theta$, taking as input a data matrix $X$ and a response vector $y$. You should also understand that, from the infinitely many vectors $\\theta$, we want to find the one that minimizes the prediction error of the corresponding model.\n",
    "\n",
    "To determine the most appropriate parameter vector $\\theta$, logistic regression must solve an optimization problem. An optimization problem is one in which, given a function, we want to find the point that corresponds to the optimal value (minimum or maximum) of the function.\n",
    "\n",
    "As an illustration, the following code computes the minimum value of the function $f(x) = x^3 - 3x^2 + 7$ within the range $[3,500]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f1 = lambda x: (x ** 3)-(3 *(x ** 2))+7\n",
    "\n",
    "# Get 1000 evenly spaced num bers between -1 and 3 (arbitratil chosen to ensure steep curve)\n",
    "x = np.linspace(-0.5,3,500)\n",
    "\n",
    "# Plot the curve\n",
    "plt.plot(x, f1(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94739b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "result = optimize.minimize_scalar(f1)\n",
    "print(result.success) # check if solver was successful\n",
    "\n",
    "print('Minimum of function occurs at x = %.2f' % result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e3ff7",
   "metadata": {},
   "source": [
    "The concept of optimization is also applicable to functions of more than one variable.\n",
    "\n",
    "In the following example, a function of two variables (a paraboloid in this case) is used to illustrate the optimization procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91483570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def f_paraboloide(x, y):\n",
    "    return x ** 2 + y ** 2\n",
    "\n",
    "x = np.linspace(-6, 6, 30)\n",
    "y = np.linspace(-6, 6, 30)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f_paraboloide(X, Y)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26660b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "ax.set_title('surface');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62583ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def f2(x):\n",
    "    return (x[0]**2 + x[1]**2)\n",
    "\n",
    "optimize.minimize(f2, [2, -1], method=\"CG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c4a2b",
   "metadata": {},
   "source": [
    "In general, optimization problems are difficult, because the function in question can be complex, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca136b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def f(x, y):\n",
    "    return np.sin(np.sqrt(x ** 2 + y ** 2))\n",
    "\n",
    "x = np.linspace(-6, 6, 30)\n",
    "y = np.linspace(-6, 6, 30)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b9b3d5",
   "metadata": {},
   "source": [
    "Logistic regression has to solve an optimization problem to find the best vector of parameters given the training dataset. In particular, the type of optimization problem to be solved in the logistic regression is minimization: given a function $J(\\Theta)$ that measures the error of the classification model on the training data, the objective is to find the combination of components of $\\theta$ such that it produces the minimum value for $J(\\Theta)$.\n",
    "\n",
    "Consider a binary classification problem in which there are $m$ examples in the training set. Also, consider that $y^{(i)} \\in \\{0.1\\}$ is the class associated with the $i$-th training example, and that $g(z_i)$ is the value of the sigmoid function corresponding to the $i$-th training example, $z_i = \\theta^T \\cdot \\mathbf{x}^{(i)}$. The cost function that logistic regression must minimize is called *cross entropy loss* or *logistic loss*. Its definition is as follows:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i = 1}^{m} [ y^{(i)} \\log (g(z_i)) + (1-y^{(i)}) \\log (1 - g(z_i))]\n",
    "$$\n",
    "\n",
    "To get an insight into the cost function above, let's consider the cost corresponding to the $i$-th training example:\n",
    "\n",
    "$$\n",
    "\\text{cost}_i = y^{(i)} \\log(g(z_i)) + (1-y^{(i)}) \\log(1 - \\sigma(z_i))\n",
    "$$\n",
    "\n",
    "In the above expression, note that the cost tends to zero when the the value values produced by the hypothesis $g(z_i)$ tends to the value of $y^{(i)}$ (ground thruth):\n",
    "\n",
    "- $\\left[(g(z_i) \\rightarrow 0) \\text{ and } (y^{(i)} = 0)\\right] \\Rightarrow (\\text{cost}_i \\rightarrow 0)$\n",
    "- $\\left[(g(z_i) \\rightarrow  1) \\text{ and } (y^{(i)} = 1)\\right] \\Rightarrow (\\text{cost}_i \\rightarrow 0)$\n",
    "\n",
    "However, if the hypothesis predicts a different value compared to $y^{(i)}$, we have one of the following two cases:\n",
    "\n",
    "- $\\left[(g(z_i) \\rightarrow 0) \\text{ and } (y^{(i)} = 1)\\right] \\Rightarrow (\\text{cost}_i \\rightarrow \\infty)$ \n",
    "- $\\left[(g(z_i) \\rightarrow 1) \\text{ and } (y^{(i)} = 0)\\right] \\Rightarrow (\\text{cost}_i \\rightarrow \\infty)$\n",
    "\n",
    "The above analysis considers only the extreme values ​​that $\\sigma(z_i)$ can take. In any case, the general conclusion we should draw is that the minimization algorithm is penalized when it selects a $\\Theta$ vector that produces the wrong prediction for the $i$-th example.\n",
    "\n",
    "The above explanation was given in the context of a single example. What the function $J(\\Theta)$ does is to compute the *average error* over all $m$ training examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447cd700",
   "metadata": {},
   "source": [
    "### Gradient Descent (and its variants)\n",
    "\n",
    "One of the most used methods to numerically perform the minimization that the logistic regression needs is [gradient descent](en.wikipedia.org/wiki/Gradient_descent). Using this method, the process of minimizing the cost function occurs in an iterative way. At each iteration, a small change is made to the components of the $\\Theta$ parameter vector:\n",
    "\n",
    "Repeat until convergence: \n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\sum_{i = 1}^{m}(g(\\theta^T \\cdot \\mathbf{x}^{(i)}) - y^{(i)}) \\, x_j^{(i)}\n",
    "$$\n",
    "\n",
    "In the update expression presented above, $\\alpha$ is a numerical value called *learning rate*. This constant controls the amount of update on the components of $\\theta$.\n",
    "\n",
    "There are numerous gradient-based optimization methods. The study of these methods is outside the scope of this course. In any case, the following animation ([source]((https://www.datasciencecentral.com/profiles/blogs/an-overview-of-gradient-descent-optimization-algorithms))) illustrates the behavior of some of these methods while minimizing a given function. notice how some of these methods manage to scape the local minima, while some others remain stuck in it.\n",
    "\n",
    "![alt text](http://storage.ning.com/topology/rest/1.0/file/get/2808327843?profile=original)\n",
    "\n",
    "The learning rate is a hyperparameter of logistic regression. The following animation (source: [Machine Learning Refined](https://github.com/jermwatt/machine_learning_refined)\n",
    ") illustrates the effect of different learning rate values ​​on the optimization process. When $\\alpha$ is defined as too large and the evaluation sequence starts to get out of control, the sequence of steps is said to diverge. On the other hand, if the value of $\\alpha$ is very small, the convergence of the process can take significantly.\n",
    "\n",
    "![alt text](https://github.com/jermwatt/machine_learning_refined/blob/gh-pages/html/gifs/steplength_1D.gif?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b144557",
   "metadata": {},
   "source": [
    "## Logistic Regression in Scikit-Learn\n",
    "\n",
    "The cells below illustrate the use of the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class.\n",
    "\n",
    "One of the parameters for instantiating the `LogisticRegression` class is called *solver*, which defines the optimization method to be used during training.\n",
    "\n",
    "*example adapted from the book [Hands-on Machine Learning with Scikit-Learn and Tensoflow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, 3:] # petal width\n",
    "y = (iris[\"target\"] == 2).astype(int) # 1 if Iris-Virginica, else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c46b8",
   "metadata": {},
   "source": [
    "The next code cell fits a logistic regression model to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffa25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "log_reg.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da5564",
   "metadata": {},
   "source": [
    "Once the model has been generated, it is possible to invoke the predict method, as shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0142e053",
   "metadata": {},
   "source": [
    "The following cell produces a graph that makes it easier to interpret the generated model. In this graph, the point where the solid and dashed curves meet corresponds to the decision boundary of the generated logistic regression model.\n",
    "\n",
    "What is the decision boundary in this example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")\n",
    "\n",
    "plt.xlabel('Petal length (cm)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a638c97a",
   "metadata": {},
   "source": [
    "The vector of coefficients of the generated logistic regression model is stored in two different attributes of the LogisticRegression class: `coef_` and `intercept_`. See the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9bae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_reg.coef_)\n",
    "print(log_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f75227",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta = [4.3, -7.19]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac9f85",
   "metadata": {},
   "source": [
    "By inspecting the above output, we can conclude that the decision boundary is:\n",
    "\n",
    "$$\n",
    "\\theta_0 + \\theta_1 \\times x_1 = 0\n",
    "$$\n",
    "\n",
    "If we replace the values in the above equation, we get:\n",
    "\n",
    "$$\n",
    "4.33 - 7.19 \\times x_1 = 0\n",
    "$$\n",
    "\n",
    "By a simple manipulation of the above expression, we can find that the decision boundary here is a vertical line that crosses $x_1 \\approx 1.66$, which is consistent with the plot shown above.\n",
    "\n",
    "In this example, notice that $x_1$ represents the only predictor used to generate the model, *petal length*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22190781",
   "metadata": {},
   "source": [
    "7.19/4.33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f669ca84",
   "metadata": {},
   "source": [
    "## Case study: Toy dataset\n",
    "\n",
    "Let us go back to our toy dataset and make build a decent classifier using Scikit-Learn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[-0.1, 1.4],\n",
    "              [-0.5,-0.1],\n",
    "              [ 1.3, 0.9],\n",
    "              [-0.6, 0.4],\n",
    "              [-1.5, 0.4],\n",
    "              [ 0.2, 0.2],\n",
    "              [-0.3,-0.4],\n",
    "              [ 0.7,-0.8],\n",
    "              [ 1.1,-1.5],\n",
    "              [-1.0, 0.9],\n",
    "              [-0.5,-1.5],\n",
    "              [-1.3,-0.4],\n",
    "              [-1.4,-1.2],\n",
    "              [-0.9,-1.1],\n",
    "              [ 0.4,-1.3],\n",
    "              [-0.4, 0.6],\n",
    "              [ 0.3,-0.5]])\n",
    "\n",
    "y = np.array([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ddbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "colormap = np.array(['r', 'b'])\n",
    "\n",
    "def plot_scatter(X, y, colormap):\n",
    "  plt.grid()\n",
    "  plt.xlim([-5.0, 5.0])\n",
    "  plt.ylim([-5.0, 5.0])\n",
    "  plt.xlabel('$x_1$', size=20)\n",
    "  plt.ylabel('$x_2$', size=20)\n",
    "  plt.title('Input 2D points', size=18)\n",
    "  plt.scatter(X[:,0], X[:, 1], s=50, c=colormap[y])\n",
    "\n",
    "plot_scatter(X, y, colormap)\n",
    "\n",
    "points_x = [x/10. for x in range(-50,+50)]\n",
    "\n",
    "bias = log_reg.intercept_\n",
    "w = log_reg.coef_.T\n",
    "points_y = [-(w[0] * x + bias)/(w[1]) for x in points_x]\n",
    "plt.plot(points_x, points_y, color = 'black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a849bc6",
   "metadata": {},
   "source": [
    "## Case study: honor dataset\n",
    "\n",
    "Let us see a complete example of creating a logistic regression model with Scikit-Learn on the honor dataset. This is a dataset that contains 200 observations about students. The feature we will use a target is `hon` which indicates if a student is an honor class or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e418dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_honor_data = pd.read_csv('https://raw.githubusercontent.com/OmaymaS/Logistic-Regression-Coefficients-Interpretation/master/honordata.csv')\n",
    "df_honor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_honor_data.drop(columns=['hon'])\n",
    "y = df_honor_data.hon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45d2a1",
   "metadata": {},
   "source": [
    "In classification, it is very common to split the available observations in two datasets: training and test. In Scikit-Learn, we can do this by using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eee513",
   "metadata": {},
   "source": [
    "The following image illustrates the effect of applying the function `train_test_split` on the data matrix $X$ and response vector $y$. From these, two data matrices and two response vectors are created. \n",
    "\n",
    "![alt text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543836883/image_6_cfpjpr.png)\n",
    "\n",
    "Please notice that, each time `train_test_split` is called, it randomly selects which examples to put in the training and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125496fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356176d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_reg.coef_)\n",
    "print(log_reg.intercept_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
